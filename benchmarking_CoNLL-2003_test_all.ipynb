{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207471af",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5b97276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using conda library 'ontogpt_fork'; naming irrelevant as this was previously going to be used to work on a different project.\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm # progress bar tracking\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 50)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# pre-processing pipeline\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NER & RE\n",
    "import spacy\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import jsonschema\n",
    "from jsonschema import validate\n",
    "\n",
    "# prompts for OpenAI\n",
    "\n",
    "import openai_secret_manager\n",
    "import openai\n",
    "\n",
    "import tiktoken\n",
    "from typing import List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f5a82",
   "metadata": {},
   "source": [
    "# Import & explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1944d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aylien = pd.read_pickle('./datasets/Aylien_68628.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046c5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename id col.\n",
    "aylien.rename(columns={'id': 'article_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dda0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68628\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 68628 entries, 0 to 68627\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype                  \n",
      "---  ------        --------------  -----                  \n",
      " 0   article_id    68628 non-null  int64                  \n",
      " 1   title         68628 non-null  object                 \n",
      " 2   published_at  68628 non-null  datetime64[ns, tzutc()]\n",
      " 3   source        68628 non-null  object                 \n",
      " 4   body          68628 non-null  object                 \n",
      "dtypes: datetime64[ns, tzutc()](1), int64(1), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "print(len(aylien))\n",
    "aylien.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e58f403",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>source</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5594565918</td>\n",
       "      <td>Alcohol Is Creating, Not Fixing, Your Anxiety</td>\n",
       "      <td>2023-04-30 04:56:45+00:00</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Opponent process theory and the self-regulatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5594560310</td>\n",
       "      <td>Petrol Subsidy Removal: Buhari hands over toug...</td>\n",
       "      <td>2023-04-30 04:55:40+00:00</td>\n",
       "      <td>Latest Nigerian News</td>\n",
       "      <td>Nigeria oil resources, especially petrol, seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5594561296</td>\n",
       "      <td>Ria Atayde on why losing weight is difficult f...</td>\n",
       "      <td>2023-04-30 04:53:55+00:00</td>\n",
       "      <td>Vietnam Explorer News Channel</td>\n",
       "      <td>Ria Atayde clapped back at body shamers as she...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  \\\n",
       "0  5594565918   \n",
       "1  5594560310   \n",
       "2  5594561296   \n",
       "\n",
       "                                               title  \\\n",
       "0      Alcohol Is Creating, Not Fixing, Your Anxiety   \n",
       "1  Petrol Subsidy Removal: Buhari hands over toug...   \n",
       "2  Ria Atayde on why losing weight is difficult f...   \n",
       "\n",
       "               published_at  \\\n",
       "0 2023-04-30 04:56:45+00:00   \n",
       "1 2023-04-30 04:55:40+00:00   \n",
       "2 2023-04-30 04:53:55+00:00   \n",
       "\n",
       "                          source  \\\n",
       "0                         Medium   \n",
       "1           Latest Nigerian News   \n",
       "2  Vietnam Explorer News Channel   \n",
       "\n",
       "                                                body  \n",
       "0  Opponent process theory and the self-regulatin...  \n",
       "1  Nigeria oil resources, especially petrol, seem...  \n",
       "2  Ria Atayde clapped back at body shamers as she...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aylien.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69b9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>source</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29776</th>\n",
       "      <td>5537090048</td>\n",
       "      <td>Tonix Pharmaceuticals Fast Forwards Its Fibrom...</td>\n",
       "      <td>2023-04-13 16:14:55+00:00</td>\n",
       "      <td>Benzinga</td>\n",
       "      <td>by\\n\\nTonix Pharmaceuticals Holding Corp TNXP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23329</th>\n",
       "      <td>5550152910</td>\n",
       "      <td>Here's why you feel breathless when you have l...</td>\n",
       "      <td>2023-04-17 17:51:26+00:00</td>\n",
       "      <td>AOL UK</td>\n",
       "      <td>People who suffer from long COVID tend to feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>5587977912</td>\n",
       "      <td>Doctor Warns That Oral Sex Is Causing An Epide...</td>\n",
       "      <td>2023-04-28 04:20:23+00:00</td>\n",
       "      <td>The Hollywood Unlocked</td>\n",
       "      <td>Doctors say oral sex is a factor in the rise o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>5585965760</td>\n",
       "      <td>How an app that tracks your coughs could save ...</td>\n",
       "      <td>2023-04-27 15:20:13+00:00</td>\n",
       "      <td>Digital Trend</td>\n",
       "      <td>“Our feeling is that cough tracking is for eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42658</th>\n",
       "      <td>5488961237</td>\n",
       "      <td>'Lessons not learned' over Aberdeen hospital p...</td>\n",
       "      <td>2023-04-06 13:27:43+00:00</td>\n",
       "      <td>BBC UK</td>\n",
       "      <td>We probably give little thought to how taps or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id  \\\n",
       "29776  5537090048   \n",
       "23329  5550152910   \n",
       "2745   5587977912   \n",
       "4143   5585965760   \n",
       "42658  5488961237   \n",
       "\n",
       "                                                   title  \\\n",
       "29776  Tonix Pharmaceuticals Fast Forwards Its Fibrom...   \n",
       "23329  Here's why you feel breathless when you have l...   \n",
       "2745   Doctor Warns That Oral Sex Is Causing An Epide...   \n",
       "4143   How an app that tracks your coughs could save ...   \n",
       "42658  'Lessons not learned' over Aberdeen hospital p...   \n",
       "\n",
       "                   published_at  \\\n",
       "29776 2023-04-13 16:14:55+00:00   \n",
       "23329 2023-04-17 17:51:26+00:00   \n",
       "2745  2023-04-28 04:20:23+00:00   \n",
       "4143  2023-04-27 15:20:13+00:00   \n",
       "42658 2023-04-06 13:27:43+00:00   \n",
       "\n",
       "                       source  \\\n",
       "29776                Benzinga   \n",
       "23329                  AOL UK   \n",
       "2745   The Hollywood Unlocked   \n",
       "4143            Digital Trend   \n",
       "42658                  BBC UK   \n",
       "\n",
       "                                                    body  \n",
       "29776  by\\n\\nTonix Pharmaceuticals Holding Corp TNXP ...  \n",
       "23329  People who suffer from long COVID tend to feel...  \n",
       "2745   Doctors say oral sex is a factor in the rise o...  \n",
       "4143   “Our feeling is that cough tracking is for eve...  \n",
       "42658  We probably give little thought to how taps or...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate 100 articles to work with:\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed_value = 2354\n",
    "\n",
    "# Set the seed\n",
    "aylien_1000 = aylien.sample(n=1000, random_state=seed_value)\n",
    "\n",
    "print(len(aylien_1000))\n",
    "aylien_1000.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615246d",
   "metadata": {},
   "source": [
    "# Load CoNLL-2003 dataset to compare NER (benchmark)\n",
    "* CoNLL-2003 is an annotated dataset used to assess NER.\n",
    "\n",
    "* load_dataset produces datafile split into train, test, validation.\n",
    "* accessible by subsetting via conll2003[\"train\"], for example, and converting to pandas df.\n",
    "* furthermore, re ner_tags:\n",
    "In the Hugging Face datasets library's CoNLL-2003 dataset, the integer values in the \"ner_tags\" column correspond to the following named entity tags:\n",
    "\n",
    "\n",
    "    * 0: \"O\" (Outside) - Indicates that a token does not belong to any named entity.\n",
    "    * 1: \"B-PER\" (Beginning of a Person entity) - Marks the beginning of a person's name.\n",
    "    * 2: \"I-PER\" (Inside of a Person entity) - Marks the continuation of a person's name.\n",
    "    * 3: \"B-ORG\" (Beginning of an Organization entity) - Marks the beginning of an organization's name.\n",
    "    * 4: \"I-ORG\" (Inside of an Organization entity) - Marks the continuation of an organization's name.\n",
    "    * 5: \"B-LOC\" (Beginning of a Location entity) - Marks the beginning of a location name.\n",
    "    * 6: \"I-LOC\" (Inside of a Location entity) - Marks the continuation of a location name.\n",
    "    * 7: \"B-MISC\" (Beginning of a Miscellaneous entity) - Marks the beginning of a miscellaneous entity.\n",
    "    * 8: \"I-MISC\" (Inside of a Miscellaneous entity) - Marks the continuation of a miscellaneous entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7d76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/David/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0133cf9fe2b14d1a8eb07f6919b84681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conll2003 = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f711093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n"
     ]
    }
   ],
   "source": [
    "train_subset = conll2003[\"train\"].select(range(len(conll2003[\"train\"])))\n",
    "conll2003_train = train_subset.to_pandas()\n",
    "print(len(conll2003_train))\n",
    "# 14,041 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b060d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3453\n"
     ]
    }
   ],
   "source": [
    "test_subset = conll2003[\"test\"].select(range(len(conll2003[\"test\"])))\n",
    "conll2003_test = test_subset.to_pandas()\n",
    "print(len(conll2003_test))\n",
    "# 3453 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08a5434c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]</td>\n",
       "      <td>[11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[22, 6, 22, 22, 23, 11]</td>\n",
       "      <td>[11, 0, 11, 12, 12, 12]</td>\n",
       "      <td>[5, 0, 5, 6, 6, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...</td>\n",
       "      <td>[0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id  \\\n",
       "0       0   \n",
       "1       1   \n",
       "2       2   \n",
       "3       3   \n",
       "4       4   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                     [Nadim, Ladki]   \n",
       "2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0      [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                            [22, 6, 22, 22, 23, 11]   \n",
       "3  [22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...   \n",
       "4  [10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0      [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                            [11, 0, 11, 12, 12, 12]   \n",
       "3  [11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...   \n",
       "4  [0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0               [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "1                                             [1, 2]  \n",
       "2                                 [5, 0, 5, 6, 6, 0]  \n",
       "3  [5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the 'id' column to 'sent_id'\n",
    "conll2003_test = conll2003_test.rename(columns={'id': 'sent_id'})\n",
    "\n",
    "conll2003_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a93735",
   "metadata": {},
   "source": [
    "### Re-map list of ints in ner_tags to the actual tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96cc480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string of input data from tokens:\n",
    "\n",
    "# Convert comma-split list to a single string\n",
    "conll2003_test['input_text'] = conll2003_test['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670a8cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]</td>\n",
       "      <td>[11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>Nadim Ladki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[22, 6, 22, 22, 23, 11]</td>\n",
       "      <td>[11, 0, 11, 12, 12, 12]</td>\n",
       "      <td>[5, 0, 5, 6, 6, 0]</td>\n",
       "      <td>AL-AIN , United Arab Emirates 1996-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Japan began the defence of their Asian Cup tit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...</td>\n",
       "      <td>[0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>But China saw their luck desert them in the se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id  \\\n",
       "0       0   \n",
       "1       1   \n",
       "2       2   \n",
       "3       3   \n",
       "4       4   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                     [Nadim, Ladki]   \n",
       "2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0      [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                            [22, 6, 22, 22, 23, 11]   \n",
       "3  [22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...   \n",
       "4  [10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0      [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                            [11, 0, 11, 12, 12, 12]   \n",
       "3  [11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...   \n",
       "4  [0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0               [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "1                                             [1, 2]   \n",
       "2                                 [5, 0, 5, 6, 6, 0]   \n",
       "3  [5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          input_text  \n",
       "0  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...  \n",
       "1                                        Nadim Ladki  \n",
       "2           AL-AIN , United Arab Emirates 1996-12-06  \n",
       "3  Japan began the defence of their Asian Cup tit...  \n",
       "4  But China saw their luck desert them in the se...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61017b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "tag_mapping = {\n",
    "    0: \"O\",\n",
    "    1: \"B-PER\",\n",
    "    2: \"I-PER\",\n",
    "    3: \"B-ORG\",\n",
    "    4: \"I-ORG\",\n",
    "    5: \"B-LOC\",\n",
    "    6: \"I-LOC\",\n",
    "    7: \"B-MISC\",\n",
    "    8: \"I-MISC\"\n",
    "}\n",
    "\n",
    "# Map the integers to the corresponding tags\n",
    "conll2003_test['mapped_ner_tags'] = conll2003_test['ner_tags'].map(lambda tags: [tag_mapping[tag] for tag in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37f2786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>mapped_ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]</td>\n",
       "      <td>[11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>Nadim Ladki</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[22, 6, 22, 22, 23, 11]</td>\n",
       "      <td>[11, 0, 11, 12, 12, 12]</td>\n",
       "      <td>[5, 0, 5, 6, 6, 0]</td>\n",
       "      <td>AL-AIN , United Arab Emirates 1996-12-06</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Japan began the defence of their Asian Cup tit...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...</td>\n",
       "      <td>[0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>But China saw their luck desert them in the se...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id  \\\n",
       "0       0   \n",
       "1       1   \n",
       "2       2   \n",
       "3       3   \n",
       "4       4   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                     [Nadim, Ladki]   \n",
       "2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0      [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                            [22, 6, 22, 22, 23, 11]   \n",
       "3  [22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...   \n",
       "4  [10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0      [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                            [11, 0, 11, 12, 12, 12]   \n",
       "3  [11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...   \n",
       "4  [0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0               [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "1                                             [1, 2]   \n",
       "2                                 [5, 0, 5, 6, 6, 0]   \n",
       "3  [5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...   \n",
       "1                                        Nadim Ladki   \n",
       "2           AL-AIN , United Arab Emirates 1996-12-06   \n",
       "3  Japan began the defence of their Asian Cup tit...   \n",
       "4  But China saw their luck desert them in the se...   \n",
       "\n",
       "                                     mapped_ner_tags  \n",
       "0       [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]  \n",
       "1                                     [B-PER, I-PER]  \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]  \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...  \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a823d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col being comma-separated list of any detected orgs.\n",
    "def extract_organizations(row):\n",
    "    tokens = row['tokens']\n",
    "    tags = row['mapped_ner_tags']\n",
    "    orgs = []\n",
    "    org = ''\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'B-ORG':\n",
    "            org = tokens[i]\n",
    "        elif tag == 'I-ORG':\n",
    "            if org:\n",
    "                org += ' ' + tokens[i]\n",
    "        else:\n",
    "            if org:\n",
    "                orgs.append(org)\n",
    "                org = ''\n",
    "    \n",
    "    if org:\n",
    "        orgs.append(org)\n",
    "    \n",
    "    return ', '.join(orgs)\n",
    "\n",
    "# Apply the function to create the 'orgs' column\n",
    "conll2003_test['orgs'] = conll2003_test.apply(extract_organizations, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac485fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>mapped_ner_tags</th>\n",
       "      <th>orgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]</td>\n",
       "      <td>[11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>Nadim Ladki</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[22, 6, 22, 22, 23, 11]</td>\n",
       "      <td>[11, 0, 11, 12, 12, 12]</td>\n",
       "      <td>[5, 0, 5, 6, 6, 0]</td>\n",
       "      <td>AL-AIN , United Arab Emirates 1996-12-06</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Japan began the defence of their Asian Cup tit...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...</td>\n",
       "      <td>[0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>But China saw their luck desert them in the se...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id  \\\n",
       "0       0   \n",
       "1       1   \n",
       "2       2   \n",
       "3       3   \n",
       "4       4   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                     [Nadim, Ladki]   \n",
       "2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0      [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                            [22, 6, 22, 22, 23, 11]   \n",
       "3  [22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...   \n",
       "4  [10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0      [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                            [11, 0, 11, 12, 12, 12]   \n",
       "3  [11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...   \n",
       "4  [0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0               [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "1                                             [1, 2]   \n",
       "2                                 [5, 0, 5, 6, 6, 0]   \n",
       "3  [5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...   \n",
       "1                                        Nadim Ladki   \n",
       "2           AL-AIN , United Arab Emirates 1996-12-06   \n",
       "3  Japan began the defence of their Asian Cup tit...   \n",
       "4  But China saw their luck desert them in the se...   \n",
       "\n",
       "                                     mapped_ner_tags  \\\n",
       "0       [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]   \n",
       "1                                     [B-PER, I-PER]   \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "  orgs  \n",
       "0       \n",
       "1       \n",
       "2       \n",
       "3       \n",
       "4       "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "236d996e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barbarians' '-' '15' '-' 'Tim' 'Stimpson' '(' 'England' ')' ';' '14' '-'\n",
      " 'Nigel' 'Walker' '(' 'Wales' ')' ',' '13' '-' 'Allan' 'Bateman' '('\n",
      " 'Wales' ')' ',' '12' '-' 'Gregor' 'Townsend' '(' 'Scotland' ')' ',' '11'\n",
      " '-' 'Tony' 'Underwood' '(' 'England' ')' ';' '10' '-' 'Rob' 'Andrew' '('\n",
      " 'England' ')' ',' '9' '-' 'Rob' 'Howley' '(' 'Wales' ')' ';' '8' '-'\n",
      " 'Scott' 'Quinnell' '(' 'Wales' ')' ',' '7' '-' 'Neil' 'Back' '('\n",
      " 'England' ')' ',' '6' '-' 'Dale' 'McIntosh' '(' 'Pontypridd' ')' ',' '5'\n",
      " '-' 'Ian' 'Jones' '(' 'New' 'Zealand' ')' ',' '4' '-' 'Craig' 'Quinnell'\n",
      " '(' 'Wales' ')' ',' '3' '-' 'Darren' 'Garforth' '(' 'Leicester' ')' ','\n",
      " '2' '-' 'Norm' 'Hewitt' '(' 'New' 'Zealand' ')' ',' '1' '-' 'Nick'\n",
      " 'Popplewell' '(' 'Ireland' ')' '.']\n",
      "['Australia' '-' '15' '-' 'Matthew' 'Burke' ';' '14' '-' 'Joe' 'Roff' ','\n",
      " '13' '-' 'Daniel' 'Herbert' ',' '12' '-' 'Tim' 'Horan' '(' 'captain' ')'\n",
      " ',' '11' '-' 'David' 'Campese' ';' '10' '-' 'Pat' 'Howard' ',' '9' '-'\n",
      " 'Sam' 'Payne' ';' '8' '-' 'Michael' 'Brial' ',' '7' '-' 'David' 'Wilson'\n",
      " ',' '6' '-' 'Owen' 'Finegan' ',' '5' '-' 'David' 'Giffin' ',' '4' '-'\n",
      " 'Tim' 'Gavin' ',' '3' '-' 'Andrew' 'Blades' ',' '2' '-' 'Marco' 'Caputo'\n",
      " ',' '1' '-' 'Dan' 'Crowley' '.']\n",
      "['South' 'Korea' ':' '1' '-' 'Kim' 'Byung' 'Ji' ';' '2' '-' 'Kim' 'Pan'\n",
      " 'Keun' ';' '5' '-' 'Huh' 'Ki' 'Tae' ';' '8' '-' 'Roh' 'Sang' 'Rae' '('\n",
      " '7' '-' 'Sin' 'Tae' 'Yong' '33' ')' ';' '9' '-' 'Kim' 'Do' 'Hoon' ';'\n",
      " '11' '-' 'Ko' 'Jeong' 'Woon' ';' '17' '-' 'Ha' 'Seok' 'Ju' ';' '18' '-'\n",
      " 'Hwang' 'Sun' 'Hong' ';' '22' '-' 'Lee' 'Young' 'Jin' ';' '23' '-' 'Yoo'\n",
      " 'Sang' 'Chul' ';' '24' '-' 'Kim' 'Joo' 'Sung' '.']\n",
      "['Syria' ':' '24' '-' 'Salem' 'Bitar' ',' '3' '-' 'Bachar' 'Srour' ';' '4'\n",
      " '-' 'Hassan' 'Abbas' ',' '5' '-' 'Tarek' 'Jabban' ',' '6' '-' 'Ammar'\n",
      " 'Awad' '(' '9' '-' 'Louay' 'Taleb' '69' ')' ',' '8' '-' 'Nihad'\n",
      " 'al-Boushi' ',' '10' '-' 'Mohammed' 'Afash' ',' '12' '-' 'Ali' 'Dib' ','\n",
      " '13' '-' 'Abdul' 'Latif' 'Helou' '(' '17' '-' 'Ammar' 'Rihawiy' '46' ')'\n",
      " ',' '14' '-' 'Khaled' 'Zaher' ';' '16' '-' 'Nader' 'Jokhadar' '.']\n",
      "['Japan' ':' '19' '-' 'Kenichi' 'Shimokawa' ',' '2' '-' 'Hiroshige'\n",
      " 'Yanagimoto' ',' '3' '-' 'Naoki' 'Soma' ',' '4' '-' 'Masami' 'Ihara' ','\n",
      " '5' '-' 'Norio' 'Omura' ',' '6' '-' 'Motohiro' 'Yamaguchi' ',' '8' '-'\n",
      " 'Masakiyo' 'Maezono' '(' '7' '-' 'Yasuto' 'Honda' '71' ')' ',' '9' '-'\n",
      " 'Takuya' 'Takagi' ',' '10' '-' 'Hiroshi' 'Nanami' ',' '11' '-'\n",
      " 'Kazuyoshi' 'Miura' ',' '15' '-' 'Hiroaki' 'Morishima' '(' '14' '-'\n",
      " 'Masayuki' 'Okano' '75' ')' '.']\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame in descending order based on the length of the list in 'tokens' column and extract the top 5 rows\n",
    "top_5_rows = conll2003_test.sort_values(by='tokens', key=lambda x: x.apply(len), ascending=False)[:5]\n",
    "\n",
    "for i in top_5_rows['tokens']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11c8f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1229\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>mapped_ner_tags</th>\n",
       "      <th>orgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>[Japan, ,, co-hosts, of, the, World, Cup, in, ...</td>\n",
       "      <td>[22, 6, 42, 15, 12, 22, 22, 15, 11, 10, 38, 16...</td>\n",
       "      <td>[11, 0, 21, 13, 11, 12, 12, 13, 11, 0, 21, 11,...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Japan , co-hosts of the World Cup in 2002 and ...</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-MISC, I-MISC, O, O, O, O...</td>\n",
       "      <td>FIFA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>[RUGBY, UNION, -, CUTTITTA, BACK, FOR, ITALY, ...</td>\n",
       "      <td>[22, 21, 8, 22, 22, 15, 16, 22, 22, 22, 7]</td>\n",
       "      <td>[11, 12, 0, 11, 12, 13, 11, 12, 12, 12, 0]</td>\n",
       "      <td>[3, 4, 0, 1, 0, 0, 5, 0, 0, 0, 0]</td>\n",
       "      <td>RUGBY UNION - CUTTITTA BACK FOR ITALY AFTER A ...</td>\n",
       "      <td>[B-ORG, I-ORG, O, B-PER, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>RUGBY UNION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>[Plymouth, 4, Exeter, 1]</td>\n",
       "      <td>[21, 11, 22, 11]</td>\n",
       "      <td>[11, 12, 12, 12]</td>\n",
       "      <td>[3, 0, 3, 0]</td>\n",
       "      <td>Plymouth 4 Exeter 1</td>\n",
       "      <td>[B-ORG, O, B-ORG, O]</td>\n",
       "      <td>Plymouth, Exeter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144</td>\n",
       "      <td>[Dutch, forward, Reggie, Blinker, had, his, in...</td>\n",
       "      <td>[16, 16, 22, 22, 38, 29, 16, 21, 40, 15, 22, 1...</td>\n",
       "      <td>[11, 12, 12, 12, 21, 11, 12, 12, 21, 13, 11, 1...</td>\n",
       "      <td>[7, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, ...</td>\n",
       "      <td>Dutch forward Reggie Blinker had his indefinit...</td>\n",
       "      <td>[B-MISC, O, B-PER, I-PER, O, O, O, O, O, O, B-...</td>\n",
       "      <td>FIFA, Sheffield Wednesday, Liverpool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145</td>\n",
       "      <td>[Blinker, missed, his, club, 's, last, two, ga...</td>\n",
       "      <td>[21, 38, 29, 21, 27, 16, 11, 24, 15, 22, 38, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 11, 12, 12, 12, 13, 11, 21, 1...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Blinker missed his club 's last two games afte...</td>\n",
       "      <td>[B-PER, O, O, O, O, O, O, O, O, B-ORG, O, O, O...</td>\n",
       "      <td>FIFA, Wednesday, Udinese, Feyenoord</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id  \\\n",
       "0      19   \n",
       "1      22   \n",
       "2     141   \n",
       "3     144   \n",
       "4     145   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Japan, ,, co-hosts, of, the, World, Cup, in, ...   \n",
       "1  [RUGBY, UNION, -, CUTTITTA, BACK, FOR, ITALY, ...   \n",
       "2                           [Plymouth, 4, Exeter, 1]   \n",
       "3  [Dutch, forward, Reggie, Blinker, had, his, in...   \n",
       "4  [Blinker, missed, his, club, 's, last, two, ga...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [22, 6, 42, 15, 12, 22, 22, 15, 11, 10, 38, 16...   \n",
       "1         [22, 21, 8, 22, 22, 15, 16, 22, 22, 22, 7]   \n",
       "2                                   [21, 11, 22, 11]   \n",
       "3  [16, 16, 22, 22, 38, 29, 16, 21, 40, 15, 22, 1...   \n",
       "4  [21, 38, 29, 21, 27, 16, 11, 24, 15, 22, 38, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0  [11, 0, 21, 13, 11, 12, 12, 13, 11, 0, 21, 11,...   \n",
       "1         [11, 12, 0, 11, 12, 13, 11, 12, 12, 12, 0]   \n",
       "2                                   [11, 12, 12, 12]   \n",
       "3  [11, 12, 12, 12, 21, 11, 12, 12, 21, 13, 11, 1...   \n",
       "4  [11, 21, 11, 12, 11, 12, 12, 12, 13, 11, 21, 1...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0  [5, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1                  [3, 4, 0, 1, 0, 0, 5, 0, 0, 0, 0]   \n",
       "2                                       [3, 0, 3, 0]   \n",
       "3  [7, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, ...   \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  Japan , co-hosts of the World Cup in 2002 and ...   \n",
       "1  RUGBY UNION - CUTTITTA BACK FOR ITALY AFTER A ...   \n",
       "2                                Plymouth 4 Exeter 1   \n",
       "3  Dutch forward Reggie Blinker had his indefinit...   \n",
       "4  Blinker missed his club 's last two games afte...   \n",
       "\n",
       "                                     mapped_ner_tags  \\\n",
       "0  [B-LOC, O, O, O, O, B-MISC, I-MISC, O, O, O, O...   \n",
       "1  [B-ORG, I-ORG, O, B-PER, O, O, B-LOC, O, O, O, O]   \n",
       "2                               [B-ORG, O, B-ORG, O]   \n",
       "3  [B-MISC, O, B-PER, I-PER, O, O, O, O, O, O, B-...   \n",
       "4  [B-PER, O, O, O, O, O, O, O, O, B-ORG, O, O, O...   \n",
       "\n",
       "                                   orgs  \n",
       "0                                  FIFA  \n",
       "1                           RUGBY UNION  \n",
       "2                      Plymouth, Exeter  \n",
       "3  FIFA, Sheffield Wednesday, Liverpool  \n",
       "4   FIFA, Wednesday, Udinese, Feyenoord  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_test_orgs = conll2003_test[conll2003_test['orgs'].str.strip().ne('')]\n",
    "\n",
    "# Reset the index\n",
    "conll2003_test_orgs = conll2003_test_orgs.reset_index(drop=True)\n",
    "\n",
    "print(len(conll2003_test_orgs))\n",
    "conll2003_test_orgs.head()\n",
    "\n",
    "# i.e. 1229 rows in test_conll2003 contain an org (at least 1 B-ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84980ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blinker' 'missed' 'his' 'club' \"'s\" 'last' 'two' 'games' 'after' 'FIFA'\n",
      " 'slapped' 'a' 'worldwide' 'ban' 'on' 'him' 'for' 'appearing' 'to' 'sign'\n",
      " 'contracts' 'for' 'both' 'Wednesday' 'and' 'Udinese' 'while' 'he' 'was'\n",
      " 'playing' 'for' 'Feyenoord' '.']\n",
      "\n",
      "\n",
      "['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "\n",
      "FIFA, Wednesday, Udinese, Feyenoord\n"
     ]
    }
   ],
   "source": [
    "index_sample = 4\n",
    "\n",
    "# Print data in 'tokens' column at index 4\n",
    "print(conll2003_test_orgs.at[index_sample, 'tokens'])\n",
    "print('\\n')\n",
    "# Print data in 'orgs' column at index 4\n",
    "print(conll2003_test_orgs.at[index_sample, 'mapped_ner_tags']) # 3-4 indicating org\n",
    "print('\\n')\n",
    "# Print data in 'orgs' column at index 4\n",
    "print(conll2003_test_orgs.at[index_sample, 'orgs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261c361",
   "metadata": {},
   "source": [
    "* Initially thought above was an error (Wednesday) however the full sentence possibly implies that it is actually an org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67648d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dutch' 'forward' 'Reggie' 'Blinker' 'had' 'his' 'indefinite'\n",
      " 'suspension' 'lifted' 'by' 'FIFA' 'on' 'Friday' 'and' 'was' 'set' 'to'\n",
      " 'make' 'his' 'Sheffield' 'Wednesday' 'comeback' 'against' 'Liverpool'\n",
      " 'on' 'Saturday' '.']\n",
      "\n",
      "\n",
      "['B-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-ORG', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "FIFA, Sheffield Wednesday, Liverpool\n"
     ]
    }
   ],
   "source": [
    "# Find the row where 'mapped_ner_tags' contains 'I-ORG'\n",
    "index_sample = conll2003_test_orgs[conll2003_test_orgs['mapped_ner_tags'\n",
    "                                                      ].apply(lambda tags: 'I-ORG' in tags)\n",
    "                                  ].index[1] # change index here for new example.\n",
    "\n",
    "# Print data in 'tokens' column at the identified index\n",
    "print(conll2003_test_orgs.at[index_sample, 'tokens'])\n",
    "print('\\n')\n",
    "\n",
    "# Print data in 'mapped_ner_tags' column at the identified index\n",
    "print(conll2003_test_orgs.at[index_sample, 'mapped_ner_tags'])\n",
    "print('\\n')\n",
    "\n",
    "# Print data in 'orgs' column at the identified index\n",
    "print(conll2003_test_orgs.at[index_sample, 'orgs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf82a6",
   "metadata": {},
   "source": [
    "* Above example confirms previous commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6668d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87080b9f",
   "metadata": {},
   "source": [
    "# Run prompt generation to detect only orgs.\n",
    "* Attempting to use similar prompt format as used in main KG_construction, for consistency.\n",
    "* Aim is to benchmark NER.\n",
    "    * Despite the main research being KGC (triple extraction) -- the triples are centres on the main organisation detected in the Aylien article.\n",
    "    * Hence it makes sense to benchmark against NER, focussing on orgs.\n",
    "    * The conll-2003 dataset is also based on news articles.\n",
    "    \n",
    "We will work from the full test dataset, also feeding in sentences that were not annotated with orgs. This way we can see if the prompt results pick up any orgs not previously detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc5075d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4def62d4",
   "metadata": {},
   "source": [
    "# Prompt generation (reduced version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1708c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3453\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>mapped_ner_tags</th>\n",
       "      <th>orgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]</td>\n",
       "      <td>[11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>Nadim Ladki</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[22, 6, 22, 22, 23, 11]</td>\n",
       "      <td>[11, 0, 11, 12, 12, 12]</td>\n",
       "      <td>[5, 0, 5, 6, 6, 0]</td>\n",
       "      <td>AL-AIN , United Arab Emirates 1996-12-06</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Japan began the defence of their Asian Cup tit...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...</td>\n",
       "      <td>[0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>But China saw their luck desert them in the se...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id  \\\n",
       "0       0   \n",
       "1       1   \n",
       "2       2   \n",
       "3       3   \n",
       "4       4   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                     [Nadim, Ladki]   \n",
       "2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0      [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                            [22, 6, 22, 22, 23, 11]   \n",
       "3  [22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...   \n",
       "4  [10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0      [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                            [11, 0, 11, 12, 12, 12]   \n",
       "3  [11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...   \n",
       "4  [0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0               [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "1                                             [1, 2]   \n",
       "2                                 [5, 0, 5, 6, 6, 0]   \n",
       "3  [5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...   \n",
       "1                                        Nadim Ladki   \n",
       "2           AL-AIN , United Arab Emirates 1996-12-06   \n",
       "3  Japan began the defence of their Asian Cup tit...   \n",
       "4  But China saw their luck desert them in the se...   \n",
       "\n",
       "                                     mapped_ner_tags  \\\n",
       "0       [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]   \n",
       "1                                     [B-PER, I-PER]   \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "  orgs  \n",
       "0       \n",
       "1       \n",
       "2       \n",
       "3       \n",
       "4       "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(conll2003_test))\n",
    "conll2003_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63048ab8",
   "metadata": {},
   "source": [
    "#### Previous prompt used in main"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07aef332",
   "metadata": {},
   "source": [
    "# Create a function to generate the prompt based on the row values\n",
    "def generate_prompt(row):\n",
    "\n",
    "    body = row['body']\n",
    "    article_id = row['article_id']\n",
    "    \n",
    "    prompt = f'''\n",
    "    For the main organisation discussed in this article (if any), give me information as follows and nothing else.\n",
    "    The article id should always be present for reference.\n",
    "    Include any government organisations discussed, if they are the main organisation discussed.\n",
    "    ###\n",
    "    <\n",
    "    article_id: {article_id}\n",
    "    org: <main organisation discussed/NA>.\n",
    "    location: <location_of_main_org/NA>\n",
    "    risk_type: <risk_type_faced_by_main_org/NA>.\n",
    "    items_sold: <comma_separated_list_of_items_sold_by_main_org/NA>.\n",
    "    service_provided: <comma_separated_list_of_services_provided_by_main_org/NA>.\n",
    "    business_relations: <comma_separated_list_of_orgs_with_business_relation_to_main_org/NA>.\n",
    "    >\n",
    "    ###\n",
    "    The article is: {body}.\n",
    "    ###\n",
    "    '''\n",
    "    \n",
    "    #return prompt.replace('\\n', '')\n",
    "    return prompt\n",
    "\n",
    "# Add a new 'prompt' column by applying the generate_prompt function to each row\n",
    "aylien_1000['prompt'] = aylien_1000.apply(generate_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d67e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate the prompt based on the row values\n",
    "def generate_prompt(row):\n",
    "\n",
    "    input_text = row['input_text']\n",
    "    sent_id = row['sent_id']\n",
    "    \n",
    "    prompt = f'''\n",
    "    For any organisations detected in this article (if any), give me information as follows and nothing else.\n",
    "    The article id should always be present for reference.\n",
    "    Include any government organisations discussed, if they are the main organisation discussed.\n",
    "    ###\n",
    "    <\n",
    "    article_id: {sent_id}\n",
    "    orgs: <comma-separated list of any organisations discussed/NA>.\n",
    "    >\n",
    "    ###\n",
    "    The article is: {input_text}.\n",
    "    ###\n",
    "    '''\n",
    "    \n",
    "    #return prompt.replace('\\n', '')\n",
    "    return prompt\n",
    "\n",
    "# Add a new 'prompt' column by applying the generate_prompt function to each row\n",
    "conll2003_test['prompt'] = conll2003_test.apply(generate_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56b933e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3453\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>mapped_ner_tags</th>\n",
       "      <th>orgs</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]</td>\n",
       "      <td>[11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>\\n    For any organisations detected in this a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>Nadim Ladki</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td></td>\n",
       "      <td>\\n    For any organisations detected in this a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[22, 6, 22, 22, 23, 11]</td>\n",
       "      <td>[11, 0, 11, 12, 12, 12]</td>\n",
       "      <td>[5, 0, 5, 6, 6, 0]</td>\n",
       "      <td>AL-AIN , United Arab Emirates 1996-12-06</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td></td>\n",
       "      <td>\\n    For any organisations detected in this a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Japan began the defence of their Asian Cup tit...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td></td>\n",
       "      <td>\\n    For any organisations detected in this a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...</td>\n",
       "      <td>[0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>But China saw their luck desert them in the se...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td></td>\n",
       "      <td>\\n    For any organisations detected in this a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id  \\\n",
       "0       0   \n",
       "1       1   \n",
       "2       2   \n",
       "3       3   \n",
       "4       4   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                     [Nadim, Ladki]   \n",
       "2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0      [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                            [22, 6, 22, 22, 23, 11]   \n",
       "3  [22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...   \n",
       "4  [10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0      [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                            [11, 0, 11, 12, 12, 12]   \n",
       "3  [11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...   \n",
       "4  [0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0               [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "1                                             [1, 2]   \n",
       "2                                 [5, 0, 5, 6, 6, 0]   \n",
       "3  [5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...   \n",
       "1                                        Nadim Ladki   \n",
       "2           AL-AIN , United Arab Emirates 1996-12-06   \n",
       "3  Japan began the defence of their Asian Cup tit...   \n",
       "4  But China saw their luck desert them in the se...   \n",
       "\n",
       "                                     mapped_ner_tags  \\\n",
       "0       [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]   \n",
       "1                                     [B-PER, I-PER]   \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "  orgs  \\\n",
       "0        \n",
       "1        \n",
       "2        \n",
       "3        \n",
       "4        \n",
       "\n",
       "                                              prompt  \n",
       "0  \\n    For any organisations detected in this a...  \n",
       "1  \\n    For any organisations detected in this a...  \n",
       "2  \\n    For any organisations detected in this a...  \n",
       "3  \\n    For any organisations detected in this a...  \n",
       "4  \\n    For any organisations detected in this a...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(conll2003_test))\n",
    "conll2003_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4590ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = conll2003_test['prompt'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b56706d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    For any organisations detected in this article (if any), give me information as follows and nothing else.\\n    The article id should always be present for reference.\\n    Include any government organisations discussed, if they are the main organisation discussed.\\n    ###\\n    <\\n    article_id: 0\\n    orgs: <comma-separated list of any organisations discussed/NA>.\\n    >\\n    ###\\n    The article is: SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT ..\\n    ###\\n    '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe single prompt as example.\n",
    "\n",
    "all_prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7358ddd",
   "metadata": {},
   "source": [
    "# Exploring prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8f872",
   "metadata": {},
   "source": [
    "## Investigate average length of tokens\n",
    "\n",
    "* Important for pricing forecast and LLM API restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8829216",
   "metadata": {},
   "source": [
    "### with tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "300088e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f259c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fns to count strings in list of prompts:\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def count_tokens_in_list(prompt_list: list, encoding_name: str) -> list:\n",
    "    \"\"\"Returns a list of integers representing the number of tokens in each string in the input list.\"\"\"\n",
    "    token_counts = []\n",
    "    for prompt in prompt_list:\n",
    "        num_tokens = num_tokens_from_string(prompt, encoding_name)\n",
    "        token_counts.append(num_tokens)\n",
    "    return token_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aec8734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_name = \"cl100k_base\" # used for gpt-3.5-turbo\n",
    "token_counts = count_tokens_in_list(all_prompts, encoding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab58f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens on the smallest prompt: 87\n",
      "Number of tokens on the largest prompt: 242\n",
      "Total number of tokens for all prompts: 366751\n",
      "Average number of tokens in all_prompts: 106.21227917752678\n"
     ]
    }
   ],
   "source": [
    "min_tokens = min(token_counts)\n",
    "max_tokens = max(token_counts)\n",
    "total_tokens = sum(i for i in token_counts if isinstance(i, int))\n",
    "average_tokens = total_tokens / len(all_prompts)\n",
    "\n",
    "print(f\"Number of tokens on the smallest prompt: {min_tokens}\")\n",
    "print(f\"Number of tokens on the largest prompt: {max_tokens}\")\n",
    "print(f\"Total number of tokens for all prompts: {total_tokens}\")\n",
    "print(f\"Average number of tokens in all_prompts: {average_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07607a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimating max response tokens if prompt works correctly (update to reflect prompt used)\n",
    "\n",
    "#num_tokens_from_string(\"facing risk: yes. type of risk: thisis some text for a risk type.\", \"cl100k_base\")\n",
    "\n",
    "num_tokens_from_string(\n",
    "    \n",
    "    '''\n",
    "    article_id: {article_id}\n",
    "    org: <main organisation discussed/NA, org2, org, org, org>.\n",
    "    '''\n",
    "                       ,\"cl100k_base\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ece9ae",
   "metadata": {},
   "source": [
    "### Truncate tokens of long prompts\n",
    "\n",
    "* gpt-3.5-turbo has max tokens of 4,096 tokens\n",
    "* This includes prompt and response tokens combined.\n",
    "* response tokens should be short due to the attempt at prompt restrictions;\n",
    "    * i.e. Provide answers only in the format of <facing risk: <'yes'/'no'>. type of risk: < risk type >.> and nothing else.\n",
    "* so a generous estimate of response tokens would be 100, providing gpt-3.5-turbo successfully adheres to above prompting.\n",
    "* Therefore truncate prompt tokens to 3500 to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66c4eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_prompt(prompt: str, encoding_name: str, max_tokens: int) -> str:\n",
    "    \"\"\"Truncates a text string to the specified number of tokens.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(prompt)[:max_tokens]\n",
    "    return encoding.decode(tokens)\n",
    "\n",
    "\n",
    "def count_tokens_for_truncating(prompt_list: list, encoding_name: str, max_tokens: int) -> list:\n",
    "    \"\"\"Returns a list of strings with a maximum of max_tokens tokens.\"\"\"\n",
    "    token_counts = []\n",
    "    truncated_prompts = []\n",
    "    for prompt in prompt_list:\n",
    "        num_tokens = num_tokens_from_string(prompt, encoding_name)\n",
    "        if num_tokens > max_tokens:\n",
    "            truncated_prompt = truncate_prompt(prompt, encoding_name, max_tokens)\n",
    "            token_counts.append(max_tokens)\n",
    "        else:\n",
    "            truncated_prompt = prompt\n",
    "            token_counts.append(num_tokens)\n",
    "        truncated_prompts.append(truncated_prompt)\n",
    "    return truncated_prompts, token_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af8d00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_name = \"cl100k_base\" # used for gpt-3.5-turbo\n",
    "# encoding_name = \"r50k_base\" # used for GPT-3 models. todo check this.\n",
    "max_tokens = 3500 # Update this based on model to be used in 'Generating responses' section to correspond to token limitations.\n",
    "#all_prompts = # your list of prompts here\n",
    "\n",
    "truncated_prompts, token_counts = count_tokens_for_truncating(all_prompts, encoding_name, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76de2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens on the smallest prompt: 87\n",
      "Number of tokens on the largest prompt: 242\n",
      "Total number of tokens for all prompts: 366751\n",
      "Average number of tokens in all_prompts: 106.21227917752678\n"
     ]
    }
   ],
   "source": [
    "# Compare output to previous token counts.\n",
    "\n",
    "token_counts_truncated_prompts = count_tokens_in_list(truncated_prompts, encoding_name)\n",
    "min_tokens_truncated_prompts = min(token_counts_truncated_prompts)\n",
    "max_tokens_truncated_prompts = max(token_counts_truncated_prompts)\n",
    "total_tokens_truncated_prompts = sum(i for i in token_counts_truncated_prompts if isinstance(i, int))\n",
    "average_tokens_truncated_prompts = total_tokens_truncated_prompts / len(truncated_prompts)\n",
    "\n",
    "print(f\"Number of tokens on the smallest prompt: {min_tokens_truncated_prompts}\")\n",
    "print(f\"Number of tokens on the largest prompt: {max_tokens_truncated_prompts}\")\n",
    "print(f\"Total number of tokens for all prompts: {total_tokens_truncated_prompts}\")\n",
    "print(f\"Average number of tokens in all_prompts: {average_tokens_truncated_prompts}\")\n",
    "\n",
    "# Compare output to previous output (prior to truncation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b393c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of truncated prompts:  3453\n",
      "['\\n    For any organisations detected in this article (if any), give me information as follows and nothing else.\\n    The article id should always be present for reference.\\n    Include any government organisations discussed, if they are the main organisation discussed.\\n    ###\\n    <\\n    article_id: 0\\n    orgs: <comma-separated list of any organisations discussed/NA>.\\n    >\\n    ###\\n    The article is: SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT ..\\n    ###\\n    ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of truncated prompts: \",len(truncated_prompts))\n",
    "\n",
    "print(truncated_prompts[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2db20b",
   "metadata": {},
   "source": [
    "# Feed prompts into LLM and populate output schema directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f7c0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5609dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:  96%|█████████▋| 3327/3453 [2:38:25<3:23:01, 96.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response for prompt at index 3326: module 'openai' has no attribute 'errors'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 3453/3453 [2:44:45<00:00,  2.86s/it]  \n"
     ]
    }
   ],
   "source": [
    "# additional code to handle possible rate limits.\n",
    "import openai\n",
    "import time\n",
    "\n",
    "prompts = truncated_prompts\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "openai.api_key = os.getenv('OPENAI_KEY')\n",
    "\n",
    "\n",
    "for idx, query in enumerate(tqdm(prompts, desc=\"Processing prompts\")):\n",
    "    try:\n",
    "        response = None\n",
    "\n",
    "        while response is None:\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You answer questions in the specified format about the article I give you.\"},\n",
    "                        {\"role\": \"user\", \"content\": query},\n",
    "                    ],\n",
    "                    model=GPT_MODEL,\n",
    "                    temperature=0.5,\n",
    "                    max_tokens=400, # max tokens in response.\n",
    "                    n=1,\n",
    "                )\n",
    "            except openai.errors.TooManyRequestsError as e:\n",
    "                # Sleep for the recommended duration\n",
    "                time.sleep(e.response['Retry-After'])\n",
    "                continue\n",
    "\n",
    "        response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        responses.append(response_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response for prompt at index {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55a01874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3452\n"
     ]
    }
   ],
   "source": [
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9172e7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### \\n<article_id: 0\\norgs: Japan, China>\\n###',\n",
       " 'Sorry, I am an AI language model and I do not have access to a specific article or its content unless it is provided to me. Can you please provide me with the article you want me to analyze?',\n",
       " 'Sorry, I cannot provide the answer as the article content is not given.',\n",
       " '<article_id: 3\\norgs: Japan, Syria\\n>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15e819d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export responses data to txt file:\n",
    "# Specify the output file path\n",
    "output_file = './benchmarking/results/benchmarking_org_ner_responses.txt'\n",
    "\n",
    "# Write the list to the JSON file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(responses, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "469f60ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated schemas saved to ./benchmarking/results/benchmarking_org_ner_responses_output.json\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for response in responses:\n",
    "    # Remove < and > characters and trailing /\n",
    "    response = response.replace('<', '').replace('>', '').replace('/', '').replace('### ', '').replace('#', '')\n",
    "\n",
    "    item = {}\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        if ': ' in line:\n",
    "            key, value = line.split(': ', 1)\n",
    "            key = key.lower().strip()  # Convert key to lowercase and remove leading/trailing spaces\n",
    "            if key == 'article_id':\n",
    "                # Remove commas from value before converting to integer\n",
    "                value = value.replace(',', '')\n",
    "                item[key] = int(value)\n",
    "            elif value.strip() in [\"NA\", \"NA.\"]:\n",
    "                item[key] = \"\"\n",
    "            else:\n",
    "                item[key] = value.strip()\n",
    "    data.append(item)\n",
    "\n",
    "# Now 'data' is a list of dictionaries with lowercase keys\n",
    "\n",
    "# Specify the path and filename for the output JSON file\n",
    "output_file = \"./benchmarking/results/benchmarking_org_ner_responses_output.json\"\n",
    "\n",
    "# Write the populated_schemas list to the JSON file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(f\"Populated schemas saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf611348",
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaError",
     "evalue": "['article_id', 'orgs'] is not of type 'object', 'boolean'\n\nFailed validating 'type' in metaschema['properties']['properties']['additionalProperties']:\n    {'$id': 'http://json-schema.org/draft-07/schema#',\n     '$schema': 'http://json-schema.org/draft-07/schema#',\n     'default': True,\n     'definitions': {'nonNegativeInteger': {'minimum': 0,\n                                            'type': 'integer'},\n                     'nonNegativeIntegerDefault0': {'allOf': [{'$ref': '#/definitions/nonNegativeInteger'},\n                                                              {'default': 0}]},\n                     'schemaArray': {'items': {'$ref': '#'},\n                                     'minItems': 1,\n                                     'type': 'array'},\n                     'simpleTypes': {'enum': ['array',\n                                              'boolean',\n                                              'integer',\n                                              'null',\n                                              'number',\n                                              'object',\n                                              'string']},\n                     'stringArray': {'default': [],\n                                     'items': {'type': 'string'},\n                                     'type': 'array',\n                                     'uniqueItems': True}},\n     'properties': {'$comment': {'type': 'string'},\n                    '$id': {'format': 'uri-reference', 'type': 'string'},\n                    '$ref': {'format': 'uri-reference', 'type': 'string'},\n                    '$schema': {'format': 'uri', 'type': 'string'},\n                    'additionalItems': {'$ref': '#'},\n                    'additionalProperties': {'$ref': '#'},\n                    'allOf': {'$ref': '#/definitions/schemaArray'},\n                    'anyOf': {'$ref': '#/definitions/schemaArray'},\n                    'const': True,\n                    'contains': {'$ref': '#'},\n                    'contentEncoding': {'type': 'string'},\n                    'contentMediaType': {'type': 'string'},\n                    'default': True,\n                    'definitions': {'additionalProperties': {'$ref': '#'},\n                                    'default': {},\n                                    'type': 'object'},\n                    'dependencies': {'additionalProperties': {'anyOf': [{'$ref': '#'},\n                                                                        {'$ref': '#/definitions/stringArray'}]},\n                                     'type': 'object'},\n                    'description': {'type': 'string'},\n                    'else': {'$ref': '#'},\n                    'enum': {'items': True, 'type': 'array'},\n                    'examples': {'items': True, 'type': 'array'},\n                    'exclusiveMaximum': {'type': 'number'},\n                    'exclusiveMinimum': {'type': 'number'},\n                    'format': {'type': 'string'},\n                    'if': {'$ref': '#'},\n                    'items': {'anyOf': [{'$ref': '#'},\n                                        {'$ref': '#/definitions/schemaArray'}],\n                              'default': True},\n                    'maxItems': {'$ref': '#/definitions/nonNegativeInteger'},\n                    'maxLength': {'$ref': '#/definitions/nonNegativeInteger'},\n                    'maxProperties': {'$ref': '#/definitions/nonNegativeInteger'},\n                    'maximum': {'type': 'number'},\n                    'minItems': {'$ref': '#/definitions/nonNegativeIntegerDefault0'},\n                    'minLength': {'$ref': '#/definitions/nonNegativeIntegerDefault0'},\n                    'minProperties': {'$ref': '#/definitions/nonNegativeIntegerDefault0'},\n                    'minimum': {'type': 'number'},\n                    'multipleOf': {'exclusiveMinimum': 0, 'type': 'number'},\n                    'not': {'$ref': '#'},\n                    'oneOf': {'$ref': '#/definitions/schemaArray'},\n                    'pattern': {'format': 'regex', 'type': 'string'},\n                    'patternProperties': {'additionalProperties': {'$ref': '#'},\n                                          'default': {},\n                                          'propertyNames': {'format': 'regex'},\n                                          'type': 'object'},\n                    'properties': {'additionalProperties': {'$ref': '#'},\n                                   'default': {},\n                                   'type': 'object'},\n                    'propertyNames': {'$ref': '#'},\n                    'readOnly': {'default': False, 'type': 'boolean'},\n                    'required': {'$ref': '#/definitions/stringArray'},\n                    'then': {'$ref': '#'},\n                    'title': {'type': 'string'},\n                    'type': {'anyOf': [{'$ref': '#/definitions/simpleTypes'},\n                                       {'items': {'$ref': '#/definitions/simpleTypes'},\n                                        'minItems': 1,\n                                        'type': 'array',\n                                        'uniqueItems': True}]},\n                    'uniqueItems': {'default': False, 'type': 'boolean'}},\n     'title': 'Core schema meta-schema',\n     'type': ['object', 'boolean']}\n\nOn schema['properties']['required']:\n    ['article_id', 'orgs']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m jsonschema\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mValidationError \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation error for item at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\ontogpt_fork\\lib\\site-packages\\jsonschema\\validators.py:1117\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(instance, schema, cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m validator_for(schema)\n\u001b[1;32m-> 1117\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1118\u001b[0m validator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(schema, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1119\u001b[0m error \u001b[38;5;241m=\u001b[39m exceptions\u001b[38;5;241m.\u001b[39mbest_match(validator\u001b[38;5;241m.\u001b[39miter_errors(instance))\n",
      "File \u001b[1;32m~\\.conda\\envs\\ontogpt_fork\\lib\\site-packages\\jsonschema\\validators.py:231\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.check_schema\u001b[1;34m(cls, schema, format_checker)\u001b[0m\n\u001b[0;32m    226\u001b[0m validator \u001b[38;5;241m=\u001b[39m Validator(\n\u001b[0;32m    227\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mMETA_SCHEMA,\n\u001b[0;32m    228\u001b[0m     format_checker\u001b[38;5;241m=\u001b[39mformat_checker,\n\u001b[0;32m    229\u001b[0m )\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m validator\u001b[38;5;241m.\u001b[39miter_errors(schema):\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mSchemaError\u001b[38;5;241m.\u001b[39mcreate_from(error)\n",
      "\u001b[1;31mSchemaError\u001b[0m: ['article_id', 'orgs'] is not of type 'object', 'boolean'\n\nFailed validating 'type' in metaschema['properties']['properties']['additionalProperties']:\n    {'$id': 'http://json-schema.org/draft-07/schema#',\n     '$schema': 'http://json-schema.org/draft-07/schema#',\n     'default': True,\n     'definitions': {'nonNegativeInteger': {'minimum': 0,\n                                            'type': 'integer'},\n                     'nonNegativeIntegerDefault0': {'allOf': [{'$ref': '#/definitions/nonNegativeInteger'},\n                                                              {'default': 0}]},\n                     'schemaArray': {'items': {'$ref': '#'},\n                                     'minItems': 1,\n                                     'type': 'array'},\n                     'simpleTypes': {'enum': ['array',\n                                              'boolean',\n                                              'integer',\n                                              'null',\n                                              'number',\n                                              'object',\n                                              'string']},\n                     'stringArray': {'default': [],\n                                     'items': {'type': 'string'},\n                                     'type': 'array',\n                                     'uniqueItems': True}},\n     'properties': {'$comment': {'type': 'string'},\n                    '$id': {'format': 'uri-reference', 'type': 'string'},\n                    '$ref': {'format': 'uri-reference', 'type': 'string'},\n                    '$schema': {'format': 'uri', 'type': 'string'},\n                    'additionalItems': {'$ref': '#'},\n                    'additionalProperties': {'$ref': '#'},\n                    'allOf': {'$ref': '#/definitions/schemaArray'},\n                    'anyOf': {'$ref': '#/definitions/schemaArray'},\n                    'const': True,\n                    'contains': {'$ref': '#'},\n                    'contentEncoding': {'type': 'string'},\n                    'contentMediaType': {'type': 'string'},\n                    'default': True,\n                    'definitions': {'additionalProperties': {'$ref': '#'},\n                                    'default': {},\n                                    'type': 'object'},\n                    'dependencies': {'additionalProperties': {'anyOf': [{'$ref': '#'},\n                                                                        {'$ref': '#/definitions/stringArray'}]},\n                                     'type': 'object'},\n                    'description': {'type': 'string'},\n                    'else': {'$ref': '#'},\n                    'enum': {'items': True, 'type': 'array'},\n                    'examples': {'items': True, 'type': 'array'},\n                    'exclusiveMaximum': {'type': 'number'},\n                    'exclusiveMinimum': {'type': 'number'},\n                    'format': {'type': 'string'},\n                    'if': {'$ref': '#'},\n                    'items': {'anyOf': [{'$ref': '#'},\n                                        {'$ref': '#/definitions/schemaArray'}],\n                              'default': True},\n                    'maxItems': {'$ref': '#/definitions/nonNegativeInteger'},\n                    'maxLength': {'$ref': '#/definitions/nonNegativeInteger'},\n                    'maxProperties': {'$ref': '#/definitions/nonNegativeInteger'},\n                    'maximum': {'type': 'number'},\n                    'minItems': {'$ref': '#/definitions/nonNegativeIntegerDefault0'},\n                    'minLength': {'$ref': '#/definitions/nonNegativeIntegerDefault0'},\n                    'minProperties': {'$ref': '#/definitions/nonNegativeIntegerDefault0'},\n                    'minimum': {'type': 'number'},\n                    'multipleOf': {'exclusiveMinimum': 0, 'type': 'number'},\n                    'not': {'$ref': '#'},\n                    'oneOf': {'$ref': '#/definitions/schemaArray'},\n                    'pattern': {'format': 'regex', 'type': 'string'},\n                    'patternProperties': {'additionalProperties': {'$ref': '#'},\n                                          'default': {},\n                                          'propertyNames': {'format': 'regex'},\n                                          'type': 'object'},\n                    'properties': {'additionalProperties': {'$ref': '#'},\n                                   'default': {},\n                                   'type': 'object'},\n                    'propertyNames': {'$ref': '#'},\n                    'readOnly': {'default': False, 'type': 'boolean'},\n                    'required': {'$ref': '#/definitions/stringArray'},\n                    'then': {'$ref': '#'},\n                    'title': {'type': 'string'},\n                    'type': {'anyOf': [{'$ref': '#/definitions/simpleTypes'},\n                                       {'items': {'$ref': '#/definitions/simpleTypes'},\n                                        'minItems': 1,\n                                        'type': 'array',\n                                        'uniqueItems': True}]},\n                    'uniqueItems': {'default': False, 'type': 'boolean'}},\n     'title': 'Core schema meta-schema',\n     'type': ['object', 'boolean']}\n\nOn schema['properties']['required']:\n    ['article_id', 'orgs']"
     ]
    }
   ],
   "source": [
    "# validate the json file created above to the following schema.\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "schema = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"article_id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"ID of the article\"\n",
    "        },\n",
    "        \"orgs\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Main organisation discussed or 'NA' if not applicable\"\n",
    "\n",
    "    },\n",
    "    \"required\": [\"article_id\", \"orgs\"]\n",
    "}\n",
    "}\n",
    "\n",
    "# Load your JSON data\n",
    "with open('./benchmarking/results/benchmarking_org_ner_responses_output.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# For each object in data, validate against the schema\n",
    "for idx, item in enumerate(data):\n",
    "    try:\n",
    "        validate(instance=item, schema=schema)\n",
    "    except jsonschema.exceptions.ValidationError as ve:\n",
    "        print(f\"Validation error for item at index {idx}: {ve}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804d8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.656px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "316.84px",
    "left": "1543.99px",
    "right": "20px",
    "top": "120px",
    "width": "316.997px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
