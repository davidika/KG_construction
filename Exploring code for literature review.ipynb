{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ca6f36",
   "metadata": {},
   "source": [
    "# Knowledge Graph construction from unstructured text.\n",
    "\n",
    "## Literature Review.\n",
    "## Code from key research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745caa13",
   "metadata": {},
   "source": [
    "### End-to-End construction of NLP Knowledge Graph\" published in ACL-IJCNLP 2021\n",
    "* https://github.com/Ishani-Mondal/SciKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, TransformerWordEmbeddings\n",
    "embedding = TransformerWordEmbeddings('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "f=open('./data/Sameas_Hyp_Entire_Test.txt')\n",
    "content=f.read()\n",
    "\n",
    "f1=open('./data/Final_big_test.txt','w')\n",
    "f1.write('sentences1'+\"\\t\"+'type1'+'\\tsentences2\\t'+'type2'+\"\\tis_similar\"+\"\\n\")\n",
    "for line in content.split(\"\\n\"):\n",
    "    if(line!=\"\"):\n",
    "        if(line.split(\"\\t\")[0]!='sentences1'):\n",
    "            sentence1=Sentence(line.split(\"\\t\")[0])\n",
    "            embedding.embed(sentence1)\n",
    "            avg1=[]\n",
    "            for token in sentence1:\n",
    "                #print(token.embedding)\n",
    "                avg1.append(token.embedding)\n",
    "\n",
    "            avg1=sum(avg1)\n",
    "            \n",
    "            sentence2=Sentence(line.split(\"\\t\")[2])\n",
    "            embedding.embed(sentence2)\n",
    "            avg2=[]\n",
    "            for token in sentence2:\n",
    "                avg2.append(token.embedding)\n",
    "\n",
    "            avg2=sum(avg2)\n",
    "            if(float(np.dot(np.array(avg1), np.array(avg2))/(norm(np.array(avg1))*norm(np.array(avg2))))>0.85):\n",
    "                f1.write(line.split(\"\\t\")[0]+\"\\t\"+line.split(\"\\t\")[1]+\"\\t\"+line.split(\"\\t\")[2]+\"\\t\"+line.split(\"\\t\")[3]+\"\\t\"+line.split(\"\\t\")[4]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf2fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d481830",
   "metadata": {},
   "source": [
    "### Scalable Knowledge Graph Construction from Text Collections\n",
    "\n",
    "https://dstlry.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d9a92",
   "metadata": {},
   "source": [
    "### {TDMS}ci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics\n",
    "\n",
    "https://arxiv.org/abs/2101.10273\n",
    "\n",
    "https://github.com/IBM/science-result-extractor\n",
    "\n",
    "model training script: https://github.com/IBM/science-result-extractor/blob/master/data/TDMSci/modelTrainingScript/training_tdm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6b433b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4652/3357762189.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mColumnCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenEmbeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWordEmbeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStackedEmbeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlairEmbeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCharacterEmbeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertEmbeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flair'"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings, CharacterEmbeddings, BertEmbeddings\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'ner'} \n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '../conllFormat/'\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train_1500_v2.conll',\n",
    "                              test_file='dev_150_v2.conll',\n",
    "                              dev_file='test_500_v2.conll')\n",
    "\n",
    "\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    BertEmbeddings('bert-base-cased'), \t\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('../model/flair/tdm-bert',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=16,\n",
    "              max_epochs=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f3ca5",
   "metadata": {},
   "source": [
    "### Seq2KG: An End-to-End Neural Model for Domain Agnostic Knowledge Graph (not Text Graph) Construction from Text\n",
    "\n",
    "* https://proceedings.kr.org/2020/77/kr2020-0077-stewart-et-al.pdf\n",
    "* https://github.com/Michael-Stewart-Webdev/Seq2KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d33eb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4652/2692031380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategoryHierarchy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityTypingDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_to_wordpieces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordpieces_to_bert_embs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mE2EETModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_utils'"
     ]
    }
   ],
   "source": [
    "## JOINT MODEL\n",
    "\n",
    "import data_utils as dutils\n",
    "from data_utils import load_documents\n",
    "from data_utils import Vocab, CategoryHierarchy, EntityTypingDataset, batch_to_wordpieces, wordpieces_to_bert_embs\n",
    "from logger import logger\n",
    "from model import E2EETModel\n",
    "import torch.optim as optim\n",
    "\n",
    "from progress_bar import ProgressBar\n",
    "import time, json, sys\n",
    "import torch\n",
    "from evaluate import ModelEvaluator\n",
    "import numpy as np\n",
    "from sinusoidal_positional_embeddings import *\n",
    "import pandas as pd\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from config import Config, device\n",
    "cf = Config()\n",
    "\n",
    "torch.manual_seed(125)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(125)\n",
    "torch.cuda.manual_seed(125)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model, evaluating it every 10 epochs.\n",
    "def train(model, data_loaders, word_vocab, wordpiece_vocab, hierarchy_tr, hierarchy_et, ground_truth_triples, epoch_start = 1):\n",
    "\n",
    "\tlogger.info(\"Training model.\")\n",
    "\t\n",
    "\t# Set up a new Bert Client, for encoding the wordpieces\n",
    "\tbc = BertClient()\n",
    "\n",
    "\n",
    "\tmodelEvaluator = ModelEvaluator(model, data_loaders['dev'], word_vocab, wordpiece_vocab, hierarchy_tr, hierarchy_et, ground_truth_triples, cf)\n",
    "\t\n",
    "\t#optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=cf.LEARNING_RATE, momentum=0.9)\n",
    "\toptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=cf.LEARNING_RATE)#, momentum=0.9)\n",
    "\tmodel.cuda()\n",
    "\tprint(cf.LEARNING_RATE)\n",
    "\n",
    "\tnum_batches = len(data_loaders[\"train\"])\n",
    "\tmax_epochs = 1000\n",
    "\tprogress_bar = ProgressBar(num_batches = num_batches, max_epochs = max_epochs, logger = logger)\n",
    "\tavg_loss_list = []\n",
    "\n",
    "\t# Train the model\n",
    "\n",
    "\tfor epoch in range(epoch_start, max_epochs + 1):\n",
    "\t\tepoch_start_time = time.time()\n",
    "\t\tepoch_losses = []\t\n",
    "\n",
    "\t\tfor (i, (batch_x, batch_y_tr, batch_y_et, batch_z_tr, batch_z_et, _, batch_tx, _, _, _)) in enumerate(data_loaders[\"train\"]):\n",
    "\t\t\n",
    "\t\t\tif len(batch_x) < cf.BATCH_SIZE:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t\t# 1. Convert wordpiece ids into wordpiece tokens\n",
    "\t\t\twordpieces = batch_to_wordpieces(batch_x, wordpiece_vocab)\n",
    "\t\t\twordpiece_embs  = wordpieces_to_bert_embs(wordpieces, bc)\n",
    "\n",
    "\t\t\t# 2. Create sin embeddings and concatenate them to the bert embeddings\n",
    "\n",
    "\n",
    "\t\t\twordpiece_embs = wordpiece_embs.to(device)\n",
    "\t\t\tbatch_y_tr = batch_y_tr.float().to(device)\n",
    "\t\t\tbatch_y_et = batch_y_et.float().to(device)\n",
    "\t\t\tbatch_z = batch_z_tr.float().to(device)\n",
    "\n",
    "\t\t\t# 3. Feed these vectors to our model\n",
    "\t\t\t\n",
    "\t\t\tif cf.POSITIONAL_EMB_DIM > 0:\n",
    "\t\t\t\tsin_embs = SinusoidalPositionalEmbedding(embedding_dim=cf.POSITIONAL_EMB_DIM, padding_idx = 0, left_pad = True)\n",
    "\t\t\t\tsin_embs = sin_embs( torch.ones([batch_x.size()[0], batch_x.size()[1]])).to(device)\n",
    "\t\t\t\tjoined_embs = torch.cat((wordpiece_embs, sin_embs), dim=2)\n",
    "\t\t\telse:\n",
    "\t\t\t\tjoined_embs = wordpiece_embs\n",
    "\n",
    "\t\t\t# if len(batch_x) < cf.BATCH_SIZE:\n",
    "\t\t\t# \tzeros = torch.zeros((cf.BATCH_SIZE - len(batch_x), joined_embs.size()[1], joined_embs.size()[2])).to(device)\n",
    "\t\t\t# \tjoined_embs = torch.cat((joined_embs, zeros), dim=0)\n",
    "\t\t\t# \tprint(joined_embs)\n",
    "\t\t\t# \tprint(joined_embs.size())\n",
    "\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\tmodel.train()\n",
    "\n",
    "\t\t\ty_hat_tr, y_hat_et = model(joined_embs)\n",
    "\n",
    "\t\t\tloss = model.calculate_loss(y_hat_tr, y_hat_et, batch_x, batch_y_tr, batch_y_et, batch_z)\n",
    "\n",
    "\t\t\t# 4. Backpropagate\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tepoch_losses.append(loss)\n",
    "\n",
    "\t\t\t# 5. Draw the progress bar\n",
    "\t\t\tprogress_bar.draw_bar(i, epoch, epoch_start_time)\n",
    "\n",
    "\t\tavg_loss = sum(epoch_losses) / float(len(epoch_losses))\n",
    "\t\tavg_loss_list.append(avg_loss)\n",
    "\n",
    "\t\tprogress_bar.draw_completed_epoch(avg_loss, avg_loss_list, epoch, epoch_start_time)\n",
    "\n",
    "\t\tmodelEvaluator.evaluate_every_n_epochs(1, epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the ground truth triples csv into a list of lists.\n",
    "def parse_ground_truth_triples(df):\n",
    "\n",
    "\tground_truth_triples = []\n",
    "\n",
    "\tcurrent_sent_index = 0\n",
    "\tcurrent_triples = []\n",
    "\n",
    "\tnum_dev_documents = len(load_documents(cf.DEV_FILENAME))\n",
    "\n",
    "\tground_truth_triples_dict = { k: [] for k in range(num_dev_documents) }\n",
    "\n",
    "\n",
    "\tfor i, row in enumerate(df.itertuples()):\n",
    "\t\tsent_index = int(getattr(row, 'index'))\n",
    "\t\thead = getattr(row, 's1').split()\n",
    "\t\trel  = str(getattr(row, 'r')).split()\n",
    "\t\ttail = getattr(row, 's2').split()\n",
    "\t\tif sent_index not in ground_truth_triples_dict:\n",
    "\t\t\tground_truth_triples_dict[sent_index] = []\n",
    "\t\tground_truth_triples_dict[sent_index].append([head, rel, tail])\n",
    "\n",
    "\tfor k in range(num_dev_documents):\n",
    "\t\tground_truth_triples.append([])\n",
    "\t\tfor t in ground_truth_triples_dict[k]:\n",
    "\t\t\tground_truth_triples[-1].append(t)\n",
    "\t\n",
    "\treturn ground_truth_triples\n",
    "\n",
    "def main(opts):\n",
    "\n",
    "\n",
    "\tif len(opts) == 0:\n",
    "\t\traise ValueError(\"Usage: train.py <dataset>\")\n",
    "\tdataset = opts[0]\n",
    "\tif dataset not in ['cateringServices', 'automotiveEngineering', 'bbn']:\n",
    "\t\traise ValueError(\"Dataset must be either cateringServices, automotiveEngineering, or bbn.\")\n",
    "\n",
    "\tcf.load_config(dataset)\n",
    "\n",
    "\tlogger.info(\"Loading files...\")\n",
    "\n",
    "\n",
    "\tdata_loaders = dutils.load_obj_from_pkl_file('data loaders', cf.ASSET_FOLDER + '/data_loaders.pkl')\n",
    "\tword_vocab = dutils.load_obj_from_pkl_file('word vocab', cf.ASSET_FOLDER + '/word_vocab.pkl')\n",
    "\twordpiece_vocab = dutils.load_obj_from_pkl_file('wordpiece vocab', cf.ASSET_FOLDER + '/wordpiece_vocab.pkl')\n",
    "\thierarchy_tr = dutils.load_obj_from_pkl_file('hierarchy_tr', cf.ASSET_FOLDER + '/hierarchy_tr.pkl')\n",
    "\thierarchy_et = dutils.load_obj_from_pkl_file('hierarchy_et', cf.ASSET_FOLDER + '/hierarchy_et.pkl')\n",
    "\ttotal_wordpieces = dutils.load_obj_from_pkl_file('total wordpieces', cf.ASSET_FOLDER + '/total_wordpieces.pkl')\n",
    "\t\n",
    "\tground_truth_triples_df = pd.read_csv(cf.GROUND_TRUTH_TRIPLES_FILE)\n",
    "\n",
    "\tground_truth_triples = parse_ground_truth_triples(ground_truth_triples_df)\n",
    "\t\n",
    "\n",
    "\tlogger.info(\"Building model.\")\n",
    "\tmodel = E2EETModel(\tembedding_dim = cf.EMBEDDING_DIM + cf.POSITIONAL_EMB_DIM,\n",
    "\t\t\t\t\t\thidden_dim = cf.HIDDEN_DIM,\n",
    "\t\t\t\t\t\tvocab_size = len(wordpiece_vocab),\n",
    "\t\t\t\t\t\tlabel_size_tr = len(hierarchy_tr),\n",
    "\t\t\t\t\t\tlabel_size_et = len(hierarchy_et),\n",
    "\t\t\t\t\t\ttotal_wordpieces = total_wordpieces,\n",
    "\t\t\t\t\t\tmax_seq_len = cf.MAX_SENT_LEN,\n",
    "\t\t\t\t\t\tbatch_size = cf.BATCH_SIZE)\n",
    "\tmodel.cuda()\n",
    "\n",
    "\ttrain(model, data_loaders, word_vocab, wordpiece_vocab, hierarchy_tr, hierarchy_et, ground_truth_triples)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tprint('hello')\n",
    "\tmain(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d768ca04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4652/768326099.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### model.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "### model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import device\n",
    "\n",
    "torch.manual_seed(125)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed(125)\n",
    "\n",
    "\n",
    "class E2EETModel(nn.Module):\n",
    "\n",
    "\tdef init_hidden(self):\n",
    "\t\t# Before we've done anything, we dont have any hidden state.\n",
    "\t\t# Refer to the Pytorch documentation to see exactly\n",
    "\t\t# why they have this dimensionality.\n",
    "\t\t# The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "\t\t#return (torch.zeros(4, self.batch_size, self.hidden_dim, device=device),\n",
    "\t\t#\t\ttorch.zeros(4, self.batch_size, self.hidden_dim, device=device))\n",
    "\n",
    "\t\treturn (torch.zeros(4, self.batch_size, self.hidden_dim, device=device)) #GRU version\n",
    "\n",
    "\tdef __init__(self, embedding_dim, hidden_dim, vocab_size, label_size_tr, label_size_et, total_wordpieces, max_seq_len, batch_size):\n",
    "\t\tsuper(E2EETModel, self).__init__()\n",
    "\n",
    "\t\tself.embedding_dim = embedding_dim\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.label_size_tr = label_size_tr\n",
    "\t\tself.label_size_et = label_size_et\n",
    "\n",
    "\t\t#self.layer_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "\t\t#self.layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\t\n",
    "\t\t#self.dropout = nn.Dropout()\n",
    "\t\tself.hidden2tag_tr = nn.Linear(hidden_dim * 2, label_size_tr)\n",
    "\t\tself.hidden2tag_et = nn.Linear(hidden_dim * 2, label_size_et)\n",
    "\n",
    "\t\tself.hidden2tag2_tr = nn.Linear(label_size_tr, label_size_tr)\n",
    "\t\tself.hidden2tag2_et = nn.Linear(label_size_et, label_size_et)\n",
    "\t\t#self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "\t\tself.recurrent_layer_tr = nn.GRU(self.embedding_dim, self.hidden_dim, bidirectional = True, num_layers = 2, dropout = 0.5)\n",
    "\n",
    "\t\tself.recurrent_layer_et = nn.GRU(self.embedding_dim, self.hidden_dim, bidirectional = True, num_layers = 2, dropout = 0.5)\n",
    "\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\tdef forward(self, batch_x):\n",
    "\n",
    "\t\tself.hidden_tr = self.init_hidden()\n",
    "\t\tself.hidden_et = self.init_hidden()\n",
    "\n",
    "\t\t#batch_x = torch.relu(self.layer_1(batch_x))\n",
    "\t\t#batch_x = self.dropout(torch.relu(self.layer_1(batch_x)))\n",
    "\t\t#y_hat = self.hidden2tag(batch_x)\n",
    "\n",
    "\n",
    "\t\t#seq_lens = []\n",
    "\t\t#for x in batch_x:\n",
    "\t\t#\tseq_lens.append(batch_x.size()[1])\n",
    "\t\t#print(batch_x.size())\n",
    "\t\t#batch_x = torch.cat((batch_x, torch.zeros((self.batch_size - batch_x.size()[0], self.max_seq_len, self.embedding_dim)).to(device)))\n",
    "\t\t#seq_lens = [100, 100, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\t\t\n",
    "\t\t# GRU\n",
    "\t\tbatch_tr = torch.nn.utils.rnn.pack_padded_sequence(batch_x, [self.max_seq_len] * self.batch_size, batch_first=True, enforce_sorted=False)\n",
    "\t\tbatch_tr, self.hidden_tr = self.recurrent_layer_tr(batch_tr, self.hidden_tr)\n",
    "\t\tbatch_tr, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_tr, batch_first = True)\n",
    "\t\tbatch_tr = batch_tr.contiguous()\n",
    "\t\tbatch_tr = batch_tr.view(-1, batch_tr.shape[2])\n",
    "\n",
    "\n",
    "\t\tbatch_et = torch.nn.utils.rnn.pack_padded_sequence(batch_x, [self.max_seq_len] * self.batch_size, batch_first=True, enforce_sorted=False)\n",
    "\t\tbatch_et, self.hidden_et = self.recurrent_layer_tr(batch_et, self.hidden_et)\n",
    "\t\tbatch_et, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_et, batch_first = True)\n",
    "\t\tbatch_et = batch_et.contiguous()\n",
    "\t\tbatch_et = batch_et.view(-1, batch_et.shape[2])\n",
    "\n",
    "\n",
    "\t\t# Feed forward\n",
    "\t\t#batch = torch.relu(self.layer_1(batch_x))\n",
    "\t\t#batch = self.dropout(torch.relu(self.layer_1(batch_x)))\n",
    "\n",
    "\t\t#print(batch.size())\n",
    "\n",
    "\t\ty_hat_tr = torch.relu(self.hidden2tag_tr(batch_tr))\n",
    "\t\ty_hat_tr = self.hidden2tag2_tr(y_hat_tr)\t\t\n",
    "\t\ty_hat_tr = y_hat_tr.view(self.batch_size, self.max_seq_len, self.label_size_tr)\n",
    "\n",
    "\n",
    "\t\ty_hat_et = torch.relu(self.hidden2tag_et(batch_et))\n",
    "\t\ty_hat_et = self.hidden2tag2_et(y_hat_et)\t\t\n",
    "\t\ty_hat_et = y_hat_et.view(self.batch_size, self.max_seq_len, self.label_size_et)\n",
    "\n",
    "\n",
    "\t\treturn y_hat_tr, y_hat_et\n",
    "\t\t\n",
    "\n",
    "\n",
    "\tdef calculate_loss(self, y_hat_tr, y_hat_et, batch_x, batch_y_tr, batch_y_et, batch_z):\n",
    "\t\tnon_padding_indexes = torch.ByteTensor((batch_x > 0))\n",
    "\t\tloss_fn = nn.BCEWithLogitsLoss()\n",
    "\t\tloss_tr = loss_fn(y_hat_tr[non_padding_indexes], batch_y_tr[non_padding_indexes])\n",
    "\t\tloss_et = loss_fn(y_hat_et[non_padding_indexes], batch_y_et[non_padding_indexes])\n",
    "\t\treturn (loss_tr + loss_et) / 2\n",
    "\n",
    "\n",
    "\t# Predict the labels of a batch of wordpieces using a threshold of 0.5.\n",
    "\tdef predict_labels(self, preds):\n",
    "\t\t#preds_s = torch.sigmoid(preds)\t\t\n",
    "\t\thits  = (preds > 0).float()\n",
    "\t\treturn hits\n",
    "\n",
    "\t# Evaluate a given batch_x, predicting the labels.\n",
    "\tdef evaluate(self, batch_x):\n",
    "\t\tpreds_tr, preds_et = self.forward(batch_x)\n",
    "\t\treturn (self.predict_labels(preds_tr), self.predict_labels(preds_et))\n",
    "\n",
    "\n",
    "\n",
    "\t# Evaluate a given batch_x, but convert the predictions for each wordpiece into the predictions of each token using\n",
    "\t# the token_idxs_to_wp_idxs map.\n",
    "\tdef predict_token_labels(self, batch_x, token_idxs_to_wp_idxs):\n",
    "\t\tpreds_tr, preds_et = self.forward(batch_x)\n",
    "\t\tlabels = []\n",
    "\t\tfor preds in (preds_tr, preds_et):\n",
    "\n",
    "\t\t\tavg_preds = torch.zeros(list(batch_x.shape)[0], list(batch_x.shape)[1], list(preds.shape)[2])\n",
    "\t\t\n",
    "\t\t\tfor i, batch in enumerate(batch_x):\n",
    "\t\t\t\tfor j, wp_idxs in enumerate(token_idxs_to_wp_idxs[i]):\t\t\n",
    "\t\t\t\t\tavg_preds[i][j] = preds[i][wp_idxs].mean(dim=0)\n",
    "\n",
    "\t\t\tlabels.append(self.predict_labels(avg_preds))\n",
    "\t\treturn labels\n",
    "\n",
    "# 5.05 vs 4.81 (e2e, filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a074f",
   "metadata": {},
   "source": [
    "### Generating Knowledge Graphs by Employing Natural Language Processing\n",
    "* https://arxiv.org/abs/2011.01103\n",
    "* https://github.com/danilo-dessi/skg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1107a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4652/3339437270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEntityCleaner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityCleaner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatisticsRefiner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStatisticsRefiner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSelector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCSORelationshipsBuilder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRelationsBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'classes'"
     ]
    }
   ],
   "source": [
    "from classes.EntityCleaner import EntityCleaner\n",
    "from classes.StatisticsRefiner import StatisticsRefiner\n",
    "from classes.Mapper import Mapper\n",
    "from classes.Selector import Selector\n",
    "from classes.CSORelationshipsBuilder import RelationsBuilder\n",
    "from classes.BestLabelFinder import BestLabelFinder\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "import Levenshtein.StringMatcher as ls\n",
    "import datetime\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import operator\n",
    "import random\n",
    "import collections\n",
    "\n",
    "\n",
    "class GraphBuilder:\n",
    "\n",
    "\tdef __init__(self, inputFile):\n",
    "\t\tself.inputFile = inputFile\n",
    "\t\tself.inputDataFrame = None\n",
    "\t\tself.inputTexts = None\n",
    "\t\tself.entities = None\n",
    "\t\tself.relations = None\n",
    "\t\tself.g = nx.DiGraph()\n",
    "\t\tself.validEntities = set()\n",
    "\n",
    "\n",
    "\tdef loadData(self):\n",
    "\t\tself.inputDataFrame = pd.read_csv(self.inputFile)#.head(1000)\n",
    "\n",
    "\n",
    "\tdef parse(self):\n",
    "\t\tself.entities = [ast.literal_eval(e) for e in self.inputDataFrame['entities_column'].tolist()]\n",
    "\t\tself.relations = [ast.literal_eval(r) for r in self.inputDataFrame['relations_column'].tolist()]\n",
    "\t\tself.inputTexts = [ast.literal_eval(t) for t in self.inputDataFrame['sentences'].tolist()]\n",
    "\n",
    "\t\ttmp_input_texts = []\n",
    "\t\tfor paper_number in range(len(self.inputTexts)):\n",
    "\t\t\tpaper_sentences = []\n",
    "\t\t\tfor sentence_number in range(len(self.inputTexts[paper_number])):\n",
    "\t\t\t\tsentence = self.inputTexts[paper_number][sentence_number].lower()\n",
    "\t\t\t\tpaper_sentences += [sentence]\n",
    "\t\t\ttmp_input_texts += [paper_sentences]\n",
    "\t\tself.inputTexts = tmp_input_texts\n",
    "\n",
    "\t\tnewInputEntities = []\n",
    "\t\tfor eList in self.entities:\n",
    "\t\t\tnewEList = []\n",
    "\t\t\tfor eSentence in eList:\n",
    "\t\t\t\tnewESentence = []\n",
    "\t\t\t\tfor e in eSentence:\n",
    "\t\t\t\t\tnewESentence += [e.lower()]\n",
    "\t\t\t\tnewEList += [newESentence]\n",
    "\t\t\tnewInputEntities += [newEList]\n",
    "\t\tself.entities = newInputEntities\n",
    "\n",
    "\t\tnewInputRelations = []\n",
    "\t\tfor rList in self.relations:\n",
    "\t\t\tnewRList = []\n",
    "\t\t\tfor rSentence in rList:\n",
    "\t\t\t\tnewRSentence = []\n",
    "\t\t\t\tfor (s,p,o) in rSentence:\n",
    "\t\t\t\t\tnewRSentence += [(s.lower(), p.lower(), o.lower())]\n",
    "\t\t\t\tnewRList += [newRSentence]\n",
    "\t\t\tnewInputRelations += [newRList]\n",
    "\t\tself.relations = newInputRelations\n",
    "\n",
    "\n",
    "\t\t\n",
    "\tdef removeNoConnectedNodes(self):\n",
    "\t\tisolated_nodes = [n for n,d in self.g.degree() if d == 0]\n",
    "\t\tself.g.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "\n",
    "\tdef removeSelfEdges(self):\n",
    "\t\tself.g.remove_edges_from(self.g.selfloop_edges())\n",
    "\n",
    "\n",
    "\tdef validate(self):\n",
    "\t\tallEntities = [] \n",
    "\t\tfor i in range(len(self.entities)):\n",
    "\t\t\tfor eList in self.entities[i]:\n",
    "\t\t\t\tallEntities += [e for e in eList]\n",
    "\n",
    "\t\tallEntities = set(allEntities)\t\n",
    "\t\trefiner = StatisticsRefiner(allEntities, self.entities, self.relations, 2, 10)\n",
    "\t\tself.validEntities, self.entities,  self.relations = refiner.validate()\n",
    "\t\tprint('Entities after:', len(self.validEntities))\n",
    "\n",
    "\n",
    "\tdef cleanEntities(self):\n",
    "\t\tentityCleaner = EntityCleaner(self.entities, self.relations, self.validEntities)\n",
    "\t\tentityCleaner.run()\n",
    "\t\tself.entities = entityCleaner.getEntitiesCleaned()\n",
    "\t\tself.relations = entityCleaner.getRelationsCleaned()\n",
    "\t \n",
    "\n",
    "\t#BestLabelFinder module execution\n",
    "\tdef build_triples(self):\n",
    "\t\tfinder = BestLabelFinder(self.inputTexts, self.entities, self.relations)\n",
    "\t\tfinder.run()\n",
    "\t\treturn finder.get_triples()\n",
    "\n",
    "\n",
    "\t# Mapping of relations with our taxonomy using Mapper\n",
    "\tdef get_mapped_triples(self, triples):\n",
    "\t\tm = Mapper(triples)\n",
    "\t\tm.run()\n",
    "\t\treturn m.get_triples()\n",
    "\n",
    "\tdef map_on_definitive_triples(self, triples):\n",
    "\t\tm = Mapper(triples)\n",
    "\t\tm.map_on_definitive_predicates()\n",
    "\t\treturn m.get_triples()\n",
    "\n",
    "\n",
    "\n",
    "\tdef save_pandas(self, triples, destination):\n",
    "\t\tcolumns_order = ['s', 'p', 'o', 'source', 'support', 'abstracts']\n",
    "\t\tdata = [{'s' : s, 'p' : p, 'o' : o, 'source' : source, 'support' : support, 'abstracts' : list(set(abstracts))} for (s,p,o, source, support, abstracts) in triples]\n",
    "\t\tdf = pd.DataFrame(data, columns=columns_order)\n",
    "\t\tdf = df[columns_order]\n",
    "\t\tdf.to_csv(destination, sep=';')\n",
    "\n",
    "\tdef save_kg(self, triples, destination):\n",
    "\t\tcolumns_order = ['s', 'p', 'o']\n",
    "\t\tdata = [{'s' : s, 'p' : p, 'o' : o} for (s,p,o) in triples]\n",
    "\t\tdf = pd.DataFrame(data, columns=columns_order)\n",
    "\t\tdf = df[columns_order]\n",
    "\t\tdf.to_csv(destination, sep=';')\n",
    "\n",
    "\n",
    "\n",
    "\tdef build_g(self, selected_triples):\n",
    "\n",
    "\t\tid_gen = 0\n",
    "\t\tentity2id = {}\n",
    "\t\tid2entity = {}\n",
    "\n",
    "\t\t# a single id to each entity\n",
    "\t\tfor (s,p,o, source, support, abstracts) in selected_triples:\n",
    "\t\t\tif s not in entity2id:\n",
    "\t\t\t\tentity2id[s] = id_gen\n",
    "\t\t\t\tid2entity[id_gen] = s\n",
    "\t\t\t\tself.g.add_node(id_gen, label=s)\n",
    "\t\t\t\tid_gen += 1\n",
    "\t\t\tif o not in entity2id:\n",
    "\t\t\t\tentity2id[o] = id_gen\n",
    "\t\t\t\tid2entity[id_gen] = o\n",
    "\t\t\t\tself.g.add_node(id_gen, label=o)\n",
    "\t\t\t\tid_gen += 1\n",
    "\n",
    "\t\t# graph generation\n",
    "\t\tfor (s,p,o, source, support, abstracts) in selected_triples:\n",
    "\t\t\tidS = entity2id[s]\n",
    "\t\t\tidO = entity2id[o]\n",
    "\t\t\tself.g.add_edge(idS, idO, label=p, support=support, source=source)\n",
    "\n",
    "\t\t\n",
    "\tdef pipeline(self):\n",
    "\n",
    "\t\tprint('# LOAD AND PARSE DATA')\n",
    "\t\tprint(str(datetime.datetime.now()))\n",
    "\t\tself.loadData()\n",
    "\t\tself.parse()\n",
    "\t\tprint()\n",
    "\n",
    "\n",
    "\t\tprint('# ENTITIES VALIDATION')\n",
    "\t\tprint(str(datetime.datetime.now()))\n",
    "\t\tself.validate()\n",
    "\t\tprint()\n",
    "\n",
    "\t\tprint('# ENTITIES CLEANING')\n",
    "\t\tprint(str(datetime.datetime.now()))\n",
    "\t\tself.cleanEntities()\n",
    "\n",
    "\n",
    "\t\tprint('# TRIPLES GENERATION')\n",
    "\t\tprint(str(datetime.datetime.now()))\n",
    "\t\ttriples = self.build_triples()\n",
    "\t\tprint('Number of triples:', len(triples))\n",
    "\n",
    "\n",
    "\t\tprint('# TRIPLES MAPPING')\n",
    "\t\tprint(str(datetime.datetime.now()))\n",
    "\t\ttriples = self.get_mapped_triples(triples)\n",
    "\t\tprint('Number of triples:', len(triples))\n",
    "\t\tself.save_pandas(triples, 'out/all_triples.csv')\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t\tprint('# TRIPLES SELECTION')\n",
    "\t\tprint(str(datetime.datetime.now()))\n",
    "\t\ts = Selector(triples)\n",
    "\t\ts.run()\n",
    "\t\tselected_triples = s.get_selected_triples()\n",
    "\t\tdiscarded_triples = s.get_discarded_triples()\n",
    "\t\tprint('Number of selected triples:', len(selected_triples))\n",
    "\t\tselected_triples = self.map_on_definitive_triples(selected_triples)\n",
    "\t\tdiscarded_triples = self.map_on_definitive_triples(discarded_triples)\n",
    "\t\tself.save_pandas(selected_triples, 'out/selected_triples.csv')\n",
    "\t\tself.save_pandas(discarded_triples, 'out/discarded_triples.csv')\n",
    "\n",
    "\t\tprint('GRAPH BUILDING')\n",
    "\t\tprint(str(datetime.datetime.now()))\n",
    "\t\tself.build_g(selected_triples)\n",
    "\t\trb = RelationsBuilder(self.g)\n",
    "\t\trb.run()\n",
    "\n",
    "\t\tnew_triples_from_CSO = rb.get_triples()\n",
    "\n",
    "\n",
    "\t\t# select only subjet, predicte, and object information for the kg\n",
    "\t\tkg_triples = [(s,p,o) for (s,p,o,support, source, abstracts) in selected_triples]\n",
    "\t\tkg_triples += new_triples_from_CSO\n",
    "\t\tkg_triples = set(kg_triples)\n",
    "\t\tprint('### KG Triples:', len(kg_triples), '###')\n",
    "\t\tself.save_kg(kg_triples, 'triples.csv')\n",
    "\n",
    "\t\t#self.removeNoConnectedNodes()\n",
    "\t\t#self.removeSelfEdges()\n",
    "\t\t#nx.write_graphml(self.g, 'kg.graphml')\n",
    "\n",
    "\t\t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tbuilder = GraphBuilder('csv_e_r_full.csv')\n",
    "\tbuilder.pipeline()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b425ae",
   "metadata": {},
   "source": [
    "### T2KG: An End-to-End System for Creating Knowledge Graph from Unstructured Text\n",
    "\n",
    "* https://files.stample.com/browserUpload/9e312f19-d85a-48e5-8b96-73bf59d96c98\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43579e",
   "metadata": {},
   "source": [
    "### Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction\n",
    "\n",
    "* https://paperswithcode.com/paper/multi-task-identification-of-entities\n",
    "\n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c14e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(docs_sent[i]['doc_key'])? (Temp/ipykernel_4652/260256666.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\dika\\AppData\\Local\\Temp/ipykernel_4652/260256666.py\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    print docs_sent[i]['doc_key']\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(docs_sent[i]['doc_key'])?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pdb\n",
    "from operator import itemgetter\n",
    "def ReadJson(senfn, refn, nerfn , docs):\n",
    "    with open(senfn) as f:\n",
    "        docs_sent = [json.loads(jsonline) for jsonline in f.readlines()]\n",
    "    with open(refn) as f:\n",
    "        docs_re = [json.loads(jsonline) for jsonline in f.readlines()]\n",
    "    with open(nerfn) as f:\n",
    "        docs_ner = [json.loads(jsonline) for jsonline in f.readlines()]\n",
    "    for i in range(len(docs_sent)):\n",
    "        if docs_sent[i]['doc_key'] != docs_re[i]['doc_key'] or docs_sent[i]['doc_key'] != docs_re[i]['doc_key']:\n",
    "            pdb.set_trace()\n",
    "        else:\n",
    "            year = docs_sent[i]['doc_key'].split('_')[1]\n",
    "            venue = docs_sent[i]['doc_key'].split('_')[0]\n",
    "            docs[docs_sent[i]['doc_key']] = {'ner': docs_ner[i]['ner'], 'relation': docs_re[i]['relation'], 'coref':docs_re[i]['coref'], 'sentences': docs_sent[i]['sentences'], 'year': year, 'venue':venue}\n",
    "            PropgateHyponym(docs[docs_sent[i]['doc_key']])\n",
    "            Map2doc(docs[docs_sent[i]['doc_key']])\n",
    "\n",
    "\n",
    "def ReadJsonACL(senfn, docs):\n",
    "    with open(senfn) as f:\n",
    "        docs_sent = [json.loads(jsonline) for jsonline in f.readlines()]\n",
    "\n",
    "    for i in range(len(docs_sent)):\n",
    "            year = docs_sent[i]['doc_key'].split('_')[0][1:]\n",
    "            if year.startswith('0') or year.startswith('1'):\n",
    "                year = '20' + year\n",
    "            elif year.startswith('9') or year.startswith('8') or year.startswith('7'):\n",
    "                year = '19' + year\n",
    "            \n",
    "            if len(year) > 4:pdb.set_trace()\n",
    "            venue = docs_sent[i]['doc_key'].split('_')[0]\n",
    "            try:\n",
    "                year =int(year)\n",
    "            except:\n",
    "                print docs_sent[i]['doc_key']\n",
    "                return\n",
    "            docs[docs_sent[i]['doc_key']] = {'ner': docs_sent[i]['ner'], 'relation': docs_sent[i]['relations'], 'coref':[], 'sentences': docs_sent[i]['sentences'], 'year': year, 'venue':'ACL'}\n",
    "\n",
    "            Map2doc(docs[docs_sent[i]['doc_key']])\n",
    "\n",
    "def ReadJsonTitle(senfn, nerfn , docs):\n",
    "    with open(senfn) as f:\n",
    "        docs_sent = [json.loads(jsonline) for jsonline in f.readlines()]\n",
    "\n",
    "    with open(nerfn) as f:\n",
    "        docs_ner = [json.loads(jsonline) for jsonline in f.readlines()]\n",
    "    for i in range(len(docs_sent)):\n",
    "        if docs_sent[i]['doc_key'] != docs_ner[i]['doc_key']:\n",
    "            pdb.set_trace()\n",
    "        else:\n",
    "            year = docs_sent[i]['doc_key'].split('_')[1]\n",
    "            venue = docs_sent[i]['doc_key'].split('_')[0]\n",
    "            relations = CreateRelation(docs_ner[i]['ner'])\n",
    "            docs[docs_sent[i]['doc_key']] = {'ner': docs_ner[i]['ner'],'relation': relations, 'sentences': docs_sent[i]['sentences'], 'coref':[], 'year': year, 'venue':venue}\n",
    "\n",
    "            # PropgateHyponym(docs[docs_sent[i]['doc_key']])\n",
    "            Map2doc(docs[docs_sent[i]['doc_key']])\n",
    "\n",
    "def CreateRelation(sents):\n",
    "    all_rel = []\n",
    "    for sent in sents:\n",
    "        nerdir = {}\n",
    "        for ner in sent:\n",
    "            if ner[2] not in nerdir:\n",
    "                nerdir[ner[2]] = []\n",
    "            nerdir[ner[2]].append(ner[:2])\n",
    "        rels = []\n",
    "        if 'Task' in nerdir and 'Method' in nerdir:\n",
    "            for span1 in nerdir['Task']:\n",
    "                for span2 in nerdir['Method']:\n",
    "                    rel = span2 + span1 + ['USED-FOR']\n",
    "                    rels.append(rel)\n",
    "        all_rel.append(rels)\n",
    "    return all_rel\n",
    "            \n",
    "def PropgateHyponym(doc):\n",
    "    for i, sent in enumerate(doc['relation']):\n",
    "        hyp_dir = {}\n",
    "        new_rels = []\n",
    "        rel_span_sets = set()\n",
    "        for relation in sent:\n",
    "            if relation[-1] == 'HYPONYM-OF':\n",
    "\n",
    "                if tuple(relation[2:4]) in hyp_dir:\n",
    "                    hyp_dir[tuple(relation[2:4])].append(tuple(relation[:2]))\n",
    "                else:\n",
    "                    hyp_dir[tuple(relation[2:4])] = [tuple(relation[:2])]\n",
    "            rel_span_sets.add(tuple(relation[:4]))\n",
    "            rel_span_sets.add(tuple(relation[2:4] + relation[:2]))\n",
    "        if hyp_dir:\n",
    "            for relation in sent:\n",
    "                if relation[-1] == 'HYPONYM-OF':continue\n",
    "                span1 = tuple(relation[:2])\n",
    "                span2 = tuple(relation[2:4])\n",
    "                rel = relation[-1]\n",
    "                if span1 in hyp_dir:\n",
    "                    for new_span in hyp_dir[span1]:\n",
    "                        new_rel_span = list(new_span) + list(span2)\n",
    "                        if tuple(new_rel_span) not in rel_span_sets:\n",
    "                            new_rels.append(new_rel_span + [rel])\n",
    "                if span2 in hyp_dir:\n",
    "                    for new_span in hyp_dir[span2]:\n",
    "                        new_rel_span = list(span1) + list(new_span)\n",
    "                        if tuple(new_rel_span) not in rel_span_sets:\n",
    "                            new_rels.append(new_rel_span + [rel])\n",
    "            if new_rels:\n",
    "                doc['relation'][i] += new_rels\n",
    "def Map2doc(doc):\n",
    "    flat_sentences = []\n",
    "    flat_sentences_id = []\n",
    "    flat_token_id = []\n",
    "    map_token_id = []\n",
    "    i = 0\n",
    "    flat_ners = []\n",
    "    flat_relations = []\n",
    "    for idx, sent in enumerate(doc['sentences']):\n",
    "        flat_sentences += sent\n",
    "        sentids = []\n",
    "        for word in sent:\n",
    "            sentids.append(i)\n",
    "            i += 1\n",
    "        map_token_id.append(sentids)\n",
    "        for ner in doc['ner'][idx]:\n",
    "            start = sentids[ner[0]]\n",
    "            end = sentids[ner[1]]\n",
    "            flat_ners.append([start,end,ner[2]])\n",
    "        for relation in doc['relation'][idx]:\n",
    "            start1 = sentids[relation[0]]\n",
    "            end1 = sentids[relation[1]]\n",
    "            start2 = sentids[relation[2]]\n",
    "            end2 = sentids[relation[3]]\n",
    "            flat_relations.append([start1,end1,start2,end2,relation[-1]])\n",
    "    doc['sentences'] = flat_sentences\n",
    "    doc['relation'] = flat_relations\n",
    "    doc['ner'] = flat_ners\n",
    "    BuildKG(doc)\n",
    "\n",
    "def BuildKG(doc):\n",
    "    NERdir = {}\n",
    "    RELdir = {}\n",
    "    for ner in doc['ner']:\n",
    "        phrase = ' '.join(doc['sentences'][ner[0]:(ner[1]+1)])\n",
    "        if phrase == 'system' or phrase == 'systems':\n",
    "            ner[2] = 'Generic'\n",
    "        \n",
    "        NERdir[(ner[0],ner[1])] = [ner[2], phrase]\n",
    "    for relation in doc['relation']:\n",
    "        start1 = relation[0]\n",
    "        end1 = relation[1]\n",
    "        start2 = relation[2]\n",
    "        end2 = relation[3]\n",
    "        ntype1 = 'None'\n",
    "        ntype2 = 'None'\n",
    "        phrase1 = ' '.join(doc['sentences'][start1:(end1+1)])\n",
    "        phrase2 = ' '.join(doc['sentences'][start2:(end2+1)])\n",
    "        if (start1, end1) in NERdir:\n",
    "            ntype1 = NERdir[(start1, end1)][0]\n",
    "        else:\n",
    "            for ner in NERdir:\n",
    "                if (start1 > ner[0] and start1 < ner[1]) or (end1 > ner[0] and end1 < ner[1]):\n",
    "                    ntype1 = NERdir[ner][0]+'_partial'\n",
    "                break\n",
    "        if (start2, end2) in NERdir:\n",
    "            ntype2 = NERdir[(start2, end2)][0]\n",
    "        else:\n",
    "            for ner in NERdir:\n",
    "                if (start2 > ner[0] and start2 < ner[1]) or (end2 > ner[0] and end2 < ner[1]):\n",
    "                    ntype2 = NERdir[ner][0]+'_partial'\n",
    "                break\n",
    "        RELdir[(start1, end1,start2, end2)] = [relation[-1],(ntype1,ntype2),(phrase1, phrase2)]\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "    doc['NERdir'] = NERdir\n",
    "    doc['RELdir'] = RELdir\n",
    "\n",
    "    \n",
    "def sort_dict(dictionary):\n",
    "    sorted_dct = sorted(dictionary.items(), key=itemgetter(1),reverse=True)\n",
    "    return sorted_dct\n",
    "\n",
    "\n",
    "def ReadTopLst(venue_type, topnum):\n",
    "    top_dir = {}\n",
    "    top_dir_len = {}\n",
    "    top_dir_count = {}\n",
    "    types = ['Method','Task']\n",
    "\n",
    "    for ntype in types:\n",
    "        fn = './NER_analy/' + venue_type + '_' + ntype + '.rank'\n",
    "        i = 0\n",
    "        for line in open(fn):\n",
    "            if i > topnum:break\n",
    "            phrase, count = line.rstrip().split('\\t')\n",
    "            newphrase = phrase.replace('-','').replace(' ','')\n",
    "            i += 1\n",
    "            if newphrase not in top_dir_count or count > top_dir_count[newphrase][0]:\n",
    "                top_dir_len[phrase] = len(newphrase)\n",
    "                top_dir[phrase] = newphrase\n",
    "                top_dir_count[newphrase] = [count, phrase]\n",
    "            else:\n",
    "                continue\n",
    "    top_dir_len = sort_dict(top_dir_len)\n",
    "    toplst = []\n",
    "    for token in top_dir_len:\n",
    "        phrase = token[0]\n",
    "        toplst.append([top_dir[phrase], phrase])\n",
    "    return toplst\n",
    "        \n",
    "def ReadTopLsts(venue_types, topnum, acronym_dir):\n",
    "    top_dir = {}\n",
    "    top_dir_len = {}\n",
    "    top_dir_count = {}\n",
    "    types = ['Method','Task']\n",
    "    for venue_type in venue_types:\n",
    "        for ntype in types:\n",
    "            fn = './NER_analy/' + venue_type + '_' + ntype + '.rank'\n",
    "            i = 0\n",
    "            for line in open(fn):\n",
    "                if i > topnum:break\n",
    "                phrase, count = line.rstrip().split('\\t')\n",
    "                words = []\n",
    "                for word in phrase.split():\n",
    "                    if word.isupper() and word in acronym_dir and len(phrase.split()) > 1:\n",
    "                        words.append(acronym_dir[word])\n",
    "                    else:\n",
    "                        words.append(word)\n",
    "                phrase = ' '.join(words)\n",
    "                newphrase = phrase.replace('-','').replace(' ','')\n",
    "                i += 1\n",
    "                if newphrase not in top_dir_count or count > top_dir_count[newphrase][0]:\n",
    "                    top_dir_len[phrase] = len(newphrase)\n",
    "                    top_dir[phrase] = newphrase\n",
    "                    top_dir_count[newphrase] = [count, phrase]\n",
    "                else:\n",
    "                    continue\n",
    "    top_dir_len = sort_dict(top_dir_len)\n",
    "    toplst = []\n",
    "    for token in top_dir_len:\n",
    "        phrase = token[0]\n",
    "        toplst.append([top_dir[phrase], phrase])\n",
    "    return toplst\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "def NormalizedLst(docs, aspects='None', aspect_values = 'None'):\n",
    "    # aspects = ['year','venue'], aspect_values = [set('1988','2000'),set('EMNLP','ACL')]\n",
    "    ner_rankdir = {}\n",
    "    venue_set = set()\n",
    "    for doc_key in docs:\n",
    "        doc = docs[doc_key]\n",
    "        key = False\n",
    "        if aspects != 'None':\n",
    "            for i in range(len(aspects)):\n",
    "                aspect = aspects[i]\n",
    "                aspect_value = aspect_values[i]\n",
    "                if doc[aspect] not in aspect_value:\n",
    "                    key = True\n",
    "                    break\n",
    "        if key:continue\n",
    "        venue_set.add(doc['year'] + '_' + doc['venue'])\n",
    "        for span in doc['NERdir']:\n",
    "            phrase = doc['NERdir'][span]\n",
    "            if phrase[0] == 'Generic':continue\n",
    "            if phrase[1].endswith('system') or phrase[1].endswith('systems'):\n",
    "                phrase[0] = 'Task'\n",
    "            if phrase[0] not in ner_rankdir:\n",
    "                ner_rankdir[phrase[0]] = {}\n",
    "            if phrase[1] not in ner_rankdir[phrase[0]]:\n",
    "                ner_rankdir[phrase[0]][phrase[1]] = 0\n",
    "            ner_rankdir[phrase[0]][phrase[1]] += 1\n",
    "    ner_rankdir = CombineDir(ner_rankdir)\n",
    "    acronym_dir = GetAcronymDirNoType(ner_rankdir)\n",
    "    rankdir = FilterNERNotype(ner_rankdir,acronym_dir)\n",
    "    return rankdir, acronym_dir\n",
    "    # for ner_type in ner_rankdir:\n",
    "    #     ner_rankdir[ner_type] = sort_dict(ner_rankdir[ner_type])\n",
    "\n",
    "\n",
    "def NormalizePhrase(phrase, rankdir, acronym_dir, toplst):\n",
    "    replacewords = set(['model','approach','method','algorithm','technique','module', 'application','models','approachs','methods','algorithms','techniques','modules', 'applications', 'problem','problems','task','tasks', 'system', 'systems', 'score', 'scores','framework','frameworks', 'design', 'designs', 'formulation'])\n",
    "    words = []\n",
    "    phrase = phrase.split()\n",
    "    for word in phrase:\n",
    "        if word in acronym_dir and word not in set(['-LRB-','-RRB-']):\n",
    "            word = acronym_dir[word]\n",
    "        words.append(word)\n",
    "    phrase = ' '.join(words)\n",
    "    if '-LRB-' in phrase and '-RRB-' in phrase:\n",
    "    # if '-LRB-' in phrase and phrase.endswith('-RRB-'):\n",
    "        full = phrase.split('-LRB-')[0]\n",
    "        full = Lower(full)\n",
    "        \n",
    "        if full in acronym_dir:\n",
    "            full = acronym_dir[full]\n",
    "        fullnorm = full.replace('-','').replace(' ','')\n",
    "        for norm in toplst:\n",
    "            if norm[0] in fullnorm:\n",
    "                return norm[1]\n",
    "        lastword = full.split()[-1]\n",
    "        if lastword in replacewords:\n",
    "            full = ' '.join(full.split()[:-1])\n",
    "            if full in acronym_dir:\n",
    "                full = acronym_dir[full]\n",
    "\n",
    "                \n",
    "        if full in rankdir:\n",
    "            return full\n",
    "        if full.endswith('s'):\n",
    "            if full[:-1] in rankdir:\n",
    "                return full[:-1]\n",
    "        if full.endswith('es'):\n",
    "            if full[:-2] in rankdir:\n",
    "                return full[:-2]\n",
    "        return full\n",
    "    else:\n",
    "        full = Lower(phrase)\n",
    "        if full in acronym_dir:\n",
    "            full = acronym_dir[full]\n",
    "        fullnorm = full.replace('-','').replace(' ','')\n",
    "        for norm in toplst:\n",
    "            if norm[0] in fullnorm:\n",
    "                return norm[1]\n",
    "                                            \n",
    "        lastword = full.split()[-1]\n",
    "        # if lastword in replacewords:\n",
    "        #     newfull = ' '.join(full.split()[:-1])\n",
    "        #     if newfull in rankdir:\n",
    "        #         return newfull\n",
    "        if lastword in replacewords:\n",
    "            full = ' '.join(full.split()[:-1])\n",
    "\n",
    "        if full.endswith('s'):\n",
    "            if full[:-1] in rankdir:\n",
    "                \n",
    "                return full[:-1]\n",
    "        if full.endswith('es'):\n",
    "            if full[:-2] in rankdir:\n",
    "                return full[:-2]\n",
    "        if full in rankdir:\n",
    "            return full\n",
    "        return full\n",
    "        # acronym = Lower(acronym)\n",
    "    \n",
    "    \n",
    "    \n",
    "def topNER(docs, aspects='None', aspect_values = 'None'):\n",
    "    # aspects = ['year','venue'], aspect_values = [set('1988','2000'),set('EMNLP','ACL')]\n",
    "    ner_rankdir = {}\n",
    "    venue_set = set()\n",
    "    for doc_key in docs:\n",
    "        doc = docs[doc_key]\n",
    "        key = False\n",
    "        if aspects != 'None':\n",
    "            for i in range(len(aspects)):\n",
    "                aspect = aspects[i]\n",
    "                aspect_value = aspect_values[i]\n",
    "                if doc[aspect] not in aspect_value:\n",
    "                    key = True\n",
    "                    break\n",
    "        if key:continue\n",
    "        venue_set.add(doc['year'] + '_' + doc['venue'])\n",
    "        for span in doc['NERdir']:\n",
    "            phrase = doc['NERdir'][span]\n",
    "            if phrase[0] == 'Generic':continue\n",
    "            phrase[1] = Lower(phrase[1])\n",
    "            if phrase[1].endswith('system') or phrase[1].endswith('systems'):\n",
    "                phrase[0] = 'Task'\n",
    "            words = phrase[1].split(' ')\n",
    "            if phrase[0] not in ner_rankdir:\n",
    "                ner_rankdir[phrase[0]] = {}\n",
    "            if phrase[1] not in ner_rankdir[phrase[0]]:\n",
    "                ner_rankdir[phrase[0]][phrase[1]] = 0\n",
    "            ner_rankdir[phrase[0]][phrase[1]] += 1\n",
    "    acronym_dir = GetAcronymDir(ner_rankdir)\n",
    "    ner_rankdir = FilterNER(ner_rankdir,acronym_dir)\n",
    "\n",
    "    for ner_type in ner_rankdir:\n",
    "        ner_rankdir[ner_type] = sort_dict(ner_rankdir[ner_type])\n",
    "\n",
    "    print venue_set\n",
    "    return ner_rankdir\n",
    "\n",
    "def CombineDir(dct):\n",
    "    new_dct = {}\n",
    "    for aspect in dct:\n",
    "        for phrase in dct[aspect]:\n",
    "            if phrase in new_dct:\n",
    "                new_dct[phrase] += dct[aspect][phrase]\n",
    "            else:\n",
    "                new_dct[phrase] = dct[aspect][phrase]\n",
    "    return new_dct\n",
    "def GetAcronymDir(ner_rankdir):\n",
    "    acronym_counts = {}\n",
    "    for ner_type in ner_rankdir:\n",
    "        for key in ner_rankdir[ner_type]:\n",
    "            # if '-LRB-' in key and key.endswith('-RRB-'):\n",
    "            if '-LRB-' in key and '-RRB-' in key:\n",
    "                full = key.split('-LRB-')[0]\n",
    "                acronym = key.split('-LRB-')[1].split('-RRB-')[0]\n",
    "                if len(full) < len(acronym):\n",
    "                    full,acronym = acronym,full\n",
    "                full = Lower(full)\n",
    "                acronym = Lower(acronym)\n",
    "                if acronym not in acronym_counts:\n",
    "                    acronym_counts[acronym] = {full:1}\n",
    "                else:\n",
    "                    if full in acronym_counts[acronym]:\n",
    "                        acronym_counts[acronym][full] += 1\n",
    "                    else:\n",
    "                        acronym_counts[acronym][full] = 1\n",
    "    new_acronym_counts = {}\n",
    "    for acronym in acronym_counts:\n",
    "            # acronym_counts[acronym] = sort_dict(acronym_counts[acronym])[0][0]\n",
    "        acronym_counts[acronym] = MergePlural(acronym_counts[acronym])\n",
    "        sorted_result = sort_dict(acronym_counts[acronym])[0]\n",
    "        if sorted_result[1] < 2:\n",
    "            continue\n",
    "        else:\n",
    "            new_acronym_counts[acronym] = sorted_result[0]\n",
    "\n",
    "    return new_acronym_counts\n",
    "\n",
    "def GetAcronymDirNoType(ner_rankdir):\n",
    "    acronym_counts = {}\n",
    "\n",
    "    for key in ner_rankdir:\n",
    "            if '-LRB-' in key and '-RRB-' in key:\n",
    "                full = key.split('-LRB-')[0]\n",
    "                acronym = key.split('-LRB-')[1].split('-RRB-')[0]\n",
    "                if len(full) < len(acronym):\n",
    "                    full,acronym = acronym,full\n",
    "                full = Lower(full)\n",
    "                acronym = Lower(acronym)\n",
    "                if acronym not in acronym_counts:\n",
    "                    acronym_counts[acronym] = {full:1}\n",
    "                else:\n",
    "                    if full in acronym_counts[acronym]:\n",
    "                        acronym_counts[acronym][full] += 1\n",
    "                    else:\n",
    "                        acronym_counts[acronym][full] = 1\n",
    "    new_acronym_counts = {}\n",
    "    for acronym in acronym_counts:\n",
    "            # acronym_counts[acronym] = sort_dict(acronym_counts[acronym])[0][0]\n",
    "        acronym_counts[acronym] = MergePlural(acronym_counts[acronym])\n",
    "        sorted_result = sort_dict(acronym_counts[acronym])[0]\n",
    "        if sorted_result[1] < 2:\n",
    "            continue\n",
    "        else:\n",
    "            new_acronym_counts[acronym] = sorted_result[0]\n",
    "\n",
    "    return new_acronym_counts\n",
    "                \n",
    "def MergePlural(mix_dict):\n",
    "    merged_dict = {}\n",
    "    plural_mappings = {}\n",
    "    for key in mix_dict:\n",
    "        if key + 's' in mix_dict:\n",
    "            plural_mappings[key+'s'] = key\n",
    "            continue\n",
    "        if key + 'es' in mix_dict:\n",
    "            plural_mappings[key+'es'] = key\n",
    "            continue\n",
    "    for key in mix_dict:\n",
    "        if key in plural_mappings:\n",
    "            new_key = plural_mappings[key]\n",
    "            if new_key in merged_dict:\n",
    "                merged_dict[new_key] += mix_dict[key]\n",
    "            else:\n",
    "                merged_dict[new_key] = mix_dict[key]\n",
    "        else:\n",
    "            new_key = key\n",
    "            if new_key in merged_dict:\n",
    "                merged_dict[new_key] += mix_dict[key]\n",
    "            else:\n",
    "                merged_dict[new_key] = mix_dict[key]\n",
    "            \n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def FilterNER(ner_rankdir, acronym_dir):\n",
    "    mappings_all = {}\n",
    "    acro_mappings = {}\n",
    "    plural_mappings = {}\n",
    "    for ner_type in ner_rankdir:\n",
    "        for key1 in ner_rankdir[ner_type]:\n",
    "            # if '-LRB-' in key1 and key1.endswith('-RRB-'):\n",
    "            if '-LRB-' in key1 and '-RRB-' in key1:\n",
    "                full = key1.split('-LRB-')[0]\n",
    "                acronym = key1.split('-LRB-')[1].split('-RRB-')[0]\n",
    "                mappings_all[key1] = Lower(full)\n",
    "                if len(full) < len(acronym):\n",
    "                    full,acronym = acronym,full\n",
    "                acro_mappings[full] = acronym\n",
    "    new_ner_rankdir = {}\n",
    "    # replace all bracket phrases with their full name\n",
    "    for ner_type in ner_rankdir:\n",
    "        new_ner_rankdir[ner_type] = {}\n",
    "        for key1 in ner_rankdir[ner_type]:\n",
    "            if key1 in mappings_all:\n",
    "                if mappings_all[key1] in new_ner_rankdir[ner_type]:\n",
    "                    new_ner_rankdir[ner_type][mappings_all[key1]] += ner_rankdir[ner_type][key1]\n",
    "                else:\n",
    "                    if key1 in ner_rankdir[ner_type]:\n",
    "                        new_ner_rankdir[ner_type][mappings_all[key1]] = ner_rankdir[ner_type][key1]\n",
    "                    \n",
    "            else:\n",
    "                if key1 in new_ner_rankdir[ner_type]:\n",
    "                    new_ner_rankdir[ner_type][key1] += ner_rankdir[ner_type][key1]\n",
    "                else:\n",
    "                    if key1 in ner_rankdir[ner_type]:\n",
    "                        new_ner_rankdir[ner_type][key1] = ner_rankdir[ner_type][key1]\n",
    "                    \n",
    "\n",
    "    for ner_type in new_ner_rankdir:\n",
    "        for key1 in new_ner_rankdir[ner_type]:\n",
    "            if key1 + 's' in new_ner_rankdir[ner_type]:\n",
    "                plural_mappings[key1+'s'] = key1\n",
    "                continue\n",
    "            if key1 + 'es' in new_ner_rankdir[ner_type]:\n",
    "                plural_mappings[key1+'es'] = key1\n",
    "                continue\n",
    "\n",
    "\n",
    "    ner_rankdir_final = {}\n",
    "    for ner_type in new_ner_rankdir:\n",
    "        ner_rankdir_final[ner_type] = {}\n",
    "        for key in new_ner_rankdir[ner_type]:\n",
    "            new_key = Lower(key)\n",
    "            if new_key in plural_mappings:\n",
    "                single_term = plural_mappings[new_key]\n",
    "                single_term = single_term\n",
    "                if single_term in acronym_dir:\n",
    "                    single_term = acronym_dir[single_term]\n",
    "                if single_term in ner_rankdir_final[ner_type]:\n",
    "                        ner_rankdir_final[ner_type][single_term] += new_ner_rankdir[ner_type][key]\n",
    "                        \n",
    "                else:\n",
    "                        ner_rankdir_final[ner_type][single_term] = new_ner_rankdir[ner_type][key]\n",
    "\n",
    "            else:\n",
    "                lower_key = Lower(key)\n",
    "                if lower_key in acronym_dir:\n",
    "                    lower_key = acronym_dir[lower_key]\n",
    "                    if lower_key in plural_mappings:\n",
    "                        lower_key = plural_mappings[lower_key]\n",
    "                    \n",
    "                if lower_key in ner_rankdir_final[ner_type]:\n",
    "                    ner_rankdir_final[ner_type][lower_key] += new_ner_rankdir[ner_type][key]\n",
    "                else:\n",
    "                    ner_rankdir_final[ner_type][lower_key] = new_ner_rankdir[ner_type][key]\n",
    "                    \n",
    "    # pdb.set_trace()\n",
    "    rankdir = {}\n",
    "    replacewords = set(['model','approach','method','algorithm','technique','module', 'application','models','approachs','methods','algorithms','techniques','modules', 'applications', 'problem','problems','task','tasks', 'system', 'systems', 'score', 'scores', 'framework','frameworks','design', 'designs'])\n",
    "    for ner_type in ner_rankdir_final:\n",
    "        rankdir[ner_type] = {}\n",
    "        for key in ner_rankdir_final[ner_type]:\n",
    "            if not key:continue\n",
    "            words = key.split()\n",
    "            if words[-1] in replacewords:\n",
    "                if len(words) == 1:continue \n",
    "                new_phrase = ' '.join(words[:-1])\n",
    "                if new_phrase in acronym_dir:\n",
    "                    new_phrase = acronym_dir[new_phrase]\n",
    "                # if new_phrase not in rankdir[ner_type]:\n",
    "                #     rankdir[ner_type][new_phrase] = 0\n",
    "                # rankdir[ner_type][new_phrase] += ner_rankdir_final[ner_type][key]\n",
    "                if new_phrase in ner_rankdir_final[ner_type]:\n",
    "                    if new_phrase not in rankdir[ner_type]:\n",
    "                        rankdir[ner_type][new_phrase] = 0\n",
    "                    rankdir[ner_type][new_phrase] += ner_rankdir_final[ner_type][key]\n",
    "                else:\n",
    "                    if key not in rankdir[ner_type]:\n",
    "                        rankdir[ner_type][key] = 0\n",
    "                    rankdir[ner_type][key] += ner_rankdir_final[ner_type][key]\n",
    "                        \n",
    "            else:\n",
    "                if key not in rankdir[ner_type]:\n",
    "                    rankdir[ner_type][key] = 0\n",
    "                rankdir[ner_type][key] += ner_rankdir_final[ner_type][key]\n",
    "\n",
    "    return rankdir\n",
    "\n",
    "def FilterNERNotype(ner_rankdir, acronym_dir):\n",
    "    mappings_all = {}\n",
    "    acro_mappings = {}\n",
    "    plural_mappings = {}\n",
    "    for key1 in ner_rankdir:\n",
    "            if '-LRB-' in key1 and '-RRB-' in key1:\n",
    "                full = key1.split('-LRB-')[0]\n",
    "                acronym = key1.split('-LRB-')[1].split('-RRB-')[0]\n",
    "                \n",
    "                mappings_all[key1] = Lower(full)\n",
    "                \n",
    "    new_ner_rankdir = {}\n",
    "    # replace all bracket phrases with their full name\n",
    "\n",
    "    for key1 in ner_rankdir:\n",
    "            if key1 in mappings_all:\n",
    "                if mappings_all[key1] in new_ner_rankdir:\n",
    "                    new_ner_rankdir[mappings_all[key1]] += ner_rankdir[key1]\n",
    "                else:\n",
    "                    if key1 in ner_rankdir:\n",
    "                        new_ner_rankdir[mappings_all[key1]] = ner_rankdir[key1]\n",
    "                    \n",
    "            else:\n",
    "                if key1 in new_ner_rankdir:\n",
    "                    new_ner_rankdir[key1] += ner_rankdir[key1]\n",
    "                else:\n",
    "                    if key1 in ner_rankdir:\n",
    "                        new_ner_rankdir[key1] = ner_rankdir[key1]\n",
    "                    \n",
    "\n",
    "\n",
    "    for key1 in new_ner_rankdir:\n",
    "            if key1 + 's' in new_ner_rankdir:\n",
    "                plural_mappings[key1+'s'] = key1\n",
    "            if key1 + 'es' in new_ner_rankdir:\n",
    "                plural_mappings[key1+'es'] = key1\n",
    "\n",
    "\n",
    "\n",
    "    ner_rankdir_final = {}\n",
    "    for key in new_ner_rankdir:\n",
    "            new_key = Lower(key)\n",
    "            if new_key in plural_mappings:\n",
    "                single_term = plural_mappings[new_key]\n",
    "                single_term = Lower(single_term)\n",
    "                if single_term in acronym_dir:\n",
    "                    single_term = acronym_dir[single_term]\n",
    "                if single_term in ner_rankdir_final:\n",
    "                        ner_rankdir_final[single_term] += new_ner_rankdir[key]\n",
    "                        \n",
    "                else:\n",
    "                        ner_rankdir_final[single_term] = new_ner_rankdir[key]\n",
    "            else:\n",
    "                lower_key = new_key\n",
    "                if lower_key in acronym_dir:\n",
    "                    lower_key = acronym_dir[lower_key]\n",
    "                    if lower_key in plural_mappings:\n",
    "                        lower_key = plural_mappings[lower_key]\n",
    "                if lower_key in ner_rankdir_final:\n",
    "                    ner_rankdir_final[lower_key] += new_ner_rankdir[key]\n",
    "                else:\n",
    "                    ner_rankdir_final[lower_key] = new_ner_rankdir[key]\n",
    "                    \n",
    "\n",
    "    return ner_rankdir_final\n",
    "\n",
    "def Lower(string):\n",
    "    words = string.split()\n",
    "    new_string = []\n",
    "    for word in words:\n",
    "        if not word:continue\n",
    "        # if not word.isupper():\n",
    "        #     new_string.append(word.lower())\n",
    "        # else:\n",
    "        #     new_string.append(word)\n",
    "        if word[:1].isupper() and word[1:].islower():\n",
    "            new_string.append(word.lower())\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "    return ' '.join(new_string)\n",
    "            \n",
    "def CountMissingNER(docs):\n",
    "    allnum = 0\n",
    "    overlapnum = 0\n",
    "    nonnum = 0\n",
    "    ner_labels = [\"Task\", \"Generic\", \"Metric\", \"Material\", \"OtherScientificTerm\", \"Method\"]\n",
    "    for doc in docs:\n",
    "        for rel in docs[doc]['RELdir']:\n",
    "            rel = docs[doc]['RELdir'][rel]\n",
    "            if 'None' == rel[1][0]:\n",
    "                nonnum += 1\n",
    "                # print rel[2][0]\n",
    "            elif 'partial' in rel[1][0]:\n",
    "                overlapnum += 1\n",
    "            elif rel[1][0] not in ner_labels:\n",
    "                print rel[1][0]\n",
    "            if 'None' in rel[1][1]:\n",
    "                nonnum += 1\n",
    "            elif 'partial' in rel[1][1]:\n",
    "                overlapnum += 1\n",
    "            elif rel[1][1] not in ner_labels:\n",
    "                print rel[1][1]\n",
    "            # if rel[1][0] in ner_labels:\n",
    "            #     if rel[1][0] == 'Task':\n",
    "            #         print rel[2][0]\n",
    "            allnum += 2\n",
    "    print allnum\n",
    "    print overlapnum\n",
    "    print nonnum\n",
    "    print float(nonnum)/allnum\n",
    "    print float(overlapnum)/allnum\n",
    "\n",
    "def PrintK(dictionary, aspect, k , name):\n",
    "    strings = []\n",
    "    k = min(k,len(dictionary[aspect]))\n",
    "    for i in range(k):\n",
    "        strings.append(dictionary[aspect][i][0] + '\\t' + str(dictionary[aspect][i][1]))\n",
    "    fid = open('./NER_analy/' + name + '_'+ aspect + '.rank','w')\n",
    "    fid.write('\\n'.join(strings).encode('utf-8'))\n",
    "    fid.close()\n",
    "    \n",
    "\n",
    "def VoteRelationType(rel_dir_phrase):\n",
    "    phrase_rel_dir = {}\n",
    "    phrase_ner_dir = {}\n",
    "    phrase_count = {}\n",
    "    for rel in rel_dir_phrase:\n",
    "        for aspect in rel_dir_phrase[rel]:\n",
    "\n",
    "            for phrase in rel_dir_phrase[rel][aspect]:\n",
    "                if phrase in phrase_count:\n",
    "                    phrase_count[phrase] += rel_dir_phrase[rel][aspect][phrase]\n",
    "                else:\n",
    "                    phrase_count[phrase] = rel_dir_phrase[rel][aspect][phrase]\n",
    "                if phrase not in phrase_rel_dir:\n",
    "                    phrase_rel_dir[phrase] = {}\n",
    "                if rel not in phrase_rel_dir[phrase]:\n",
    "                    phrase_rel_dir[phrase][rel] = 0\n",
    "                if phrase not in phrase_ner_dir:\n",
    "                    phrase_ner_dir[phrase] = {}\n",
    "                if aspect not in phrase_ner_dir[phrase]:\n",
    "                    phrase_ner_dir[phrase][aspect] = 0\n",
    "                phrase_rel_dir[phrase][rel] += rel_dir_phrase[rel][aspect][phrase]\n",
    "                phrase_ner_dir[phrase][aspect] += rel_dir_phrase[rel][aspect][phrase]\n",
    "                    \n",
    "\n",
    "    new_dict = {}\n",
    "    for phrase in phrase_count:\n",
    "        phrase_rel_dir[phrase] = sort_dict(phrase_rel_dir[phrase])\n",
    "        phrase_ner_dir[phrase] = sort_dict(phrase_ner_dir[phrase])\n",
    "        rel = phrase_rel_dir[phrase][0][0]\n",
    "        aspect = phrase_ner_dir[phrase][0][0]\n",
    "        if len(phrase_rel_dir[phrase])> 1:\n",
    "            if rel == 'CONJUNCTION':\n",
    "                rel = phrase_rel_dir[phrase][1][0]\n",
    "        if len(phrase_ner_dir[phrase])> 1:\n",
    "            if aspect == 'None' or aspect == 'OtherScientificTerm':\n",
    "                aspect = phrase_ner_dir[phrase][1][0]\n",
    "            if aspect == 'None' or aspect == 'OtherScientificTerm':\n",
    "                if len(phrase_ner_dir[phrase])> 2:\n",
    "                    aspect = phrase_ner_dir[phrase][2][0]\n",
    "                \n",
    "        # for token in phrase_dir[phrase]:\n",
    "        #     if ('USED-FOR', 'Task') == token[0]:\n",
    "        #         (rel,aspect) = ('USED-FOR', 'Task')\n",
    "        #     elif ('USED-FOR_Reverse', 'Method') == token[0]:\n",
    "        #         (rel,aspect) = ('USED-FOR_Reverse', 'Method')\n",
    "                \n",
    "\n",
    "        if rel not in new_dict:\n",
    "            new_dict[rel] = {}\n",
    "        if aspect not in new_dict[rel]:\n",
    "            new_dict[rel][aspect] = {}\n",
    "        new_dict[rel][aspect][phrase] = phrase_count[phrase]\n",
    "\n",
    "    return new_dict\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "# docs = {}\n",
    "# for i in range(16):\n",
    "#     senfn = '/homes/luanyi/pubanal/data/AI2/json/'+str(i)+'.json'\n",
    "#     nerfn = '/homes/luanyi/pubanal/data/AI2/automatic_graph/results/ner_outputs/'+str(i)+'.output.json'\n",
    "#     refn = '/homes/luanyi/pubanal/data/AI2/automatic_graph/results/re_outputs/'+str(i)+'.output.json'\n",
    "#     ReadJson(senfn, refn, nerfn, docs)\n",
    "    \n",
    "# venue_sets = [set(['ICASSP','INTERSPEECH']), set(['AAAI','IJCAI']), set(['ACL','EMNLP','IJCNLP']), set(['ECCV','CVPR','ICCV']), set(['NIPS','ICML'])]\n",
    "# speech = set(['ICASSP','INTERSPEECH'])\n",
    "# AI = set(['AAAI','IJCAI'])\n",
    "# NLP = set(['ACL','EMNLP','IJCNLP'])\n",
    "# CV = set(['ECCV','CVPR','ICCV'])\n",
    "# ML = set(['NIPS','ICML'])\n",
    "# # speech = topNER(docs, ['venue'], [venue_sets[0]])\n",
    "# # AI = topNER(docs, ['venue'], [venue_sets[1]])\n",
    "# # NLP = topNER(docs, ['venue'], [venue_sets[2]])\n",
    "# # CV = topNER(docs, ['venue'], [venue_sets[3]])\n",
    "# # ML = topNER(docs, ['venue'], [venue_sets[4]])\n",
    "# b00 = set([str(i) for i in range(1990,2001)])\n",
    "# p00_to_05 = set([str(i) for i in range(2001,2006)])\n",
    "# p06_to_10 = set([str(i) for i in range(2006,2011)])\n",
    "# p10_to_17 = set([str(i) for i in range(2011,2017)])\n",
    "# # p1 = topNER(docs, ['year'], [b00])\n",
    "# # p2 = topNER(docs, ['year'], [p00_to_05])\n",
    "# # p3 = topNER(docs, ['year'], [p06_to_10])\n",
    "# # p4 = topNER(docs, ['year'], [p10_to_17])\n",
    "# # p1_speech = topNER(docs, ['year','venue'], [b00,venue_sets[0]])\n",
    "# # p2_speech = topNER(docs, ['year','venue'], [p00_to_05, venue_sets[0]])\n",
    "# # p3_speech = topNER(docs, ['year','venue'], [p06_to_10, venue_sets[0]])\n",
    "# p4_speech = topNER(docs, ['year','venue'], [p10_to_17, venue_sets[0]])\n",
    "# # p1_ai = topNER(docs, ['year','venue'], [b00,venue_sets[1]])\n",
    "# # p2_ai = topNER(docs, ['year','venue'], [p00_to_05, venue_sets[1]])\n",
    "# # p3_ai = topNER(docs, ['year','venue'], [p06_to_10, venue_sets[1]])\n",
    "# # p4_ai = topNER(docs, ['year','venue'], [p10_to_17, venue_sets[1]])\n",
    "# # p1_NLP = topNER(docs, ['year','venue'], [b00,venue_sets[2]])\n",
    "# # p2_NLP = topNER(docs, ['year','venue'], [p00_to_05, venue_sets[2]])\n",
    "# # p3_NLP = topNER(docs, ['year','venue'], [p06_to_10, venue_sets[2]])\n",
    "# # p4_NLP = topNER(docs, ['year','venue'], [p10_to_17, venue_sets[2]])\n",
    "# # p1_cv = topNER(docs, ['year','venue'], [b00,venue_sets[3]])\n",
    "# # p2_cv = topNER(docs, ['year','venue'], [p00_to_05, venue_sets[3]])\n",
    "# # p3_cv = topNER(docs, ['year','venue'], [p06_to_10, venue_sets[3]])\n",
    "# # p4_cv = topNER(docs, ['year','venue'], [p10_to_17, venue_sets[3]])\n",
    "# # p1_ml = topNER(docs, ['year','venue'], [b00,venue_sets[4]])\n",
    "# # p2_ml = topNER(docs, ['year','venue'], [p00_to_05, venue_sets[4]])\n",
    "# # p3_ml = topNER(docs, ['year','venue'], [p06_to_10, venue_sets[4]])\n",
    "# # p4_ml = topNER(docs, ['year','venue'], [p10_to_17, venue_sets[4]])\n",
    "# years = {'p1':b00, 'p2':p00_to_05, 'p3':p06_to_10, 'p4':p10_to_17}\n",
    "# venues = {'speech':speech, 'AI':AI, 'NLP':NLP, 'CV':CV, 'ML':ML}\n",
    "# ner_types = [\"Task\", \"Generic\", \"Metric\", \"Material\", \"OtherScientificTerm\", \"Method\"]\n",
    "# # rankdir, acronym_dir = NormalizedLst(docs, ['year','venue'], [years['p4'], venues['speech']])\n",
    "# # phrase = 'Gaussian mixture models'\n",
    "# # NormalizePhrase(phrase, rankdir, acronym_dir)\n",
    "\n",
    "# # pdb.set_trace()\n",
    "# print 'aa'\n",
    "# # topNER(docs, ['year','venue'], [years['p4'], venues['speech']])\n",
    "# print 'finish'\n",
    "# for year_key in years:\n",
    "#     year = years[year_key]\n",
    "#     print year\n",
    "#     for venue_key in venues:\n",
    "#         venue = venues[venue_key]\n",
    "#         x = topNER(docs, ['year','venue'], [year, venue])\n",
    "#         for ner_type in ner_types:\n",
    "#             PrintK(x,ner_type,1000, venue_key + '_' + year_key)\n",
    "        \n",
    "# for year_key in years:\n",
    "#     print year_key\n",
    "#     year = years[year_key]\n",
    "#     x = topNER(docs, ['year'], [year])\n",
    "#     for ner_type in ner_types:\n",
    "#         PrintK(x,ner_type,1000, year_key)\n",
    "# for venue_key in venues:\n",
    "#     venue = venues[venue_key]\n",
    "#     x = topNER(docs, ['venue'], [venue])\n",
    "#     for ner_type in ner_types:\n",
    "#         PrintK(x,ner_type,1000, venue_key)\n",
    "# # CountMissingNER(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc2ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
